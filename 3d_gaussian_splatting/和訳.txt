最初のところ
Radiance Field（放射場）手法は、複数の写真やビデオで撮影されたシーンの新しいビュー合成において、最近革新的な進展を遂げています。しかし、高い視覚品質を達成するためには、訓練やレンダリングに高コストなニューラルネットワークが必要であり、最近の高速化手法では、必然的に速度と品質のトレードオフが生じます。特に、孤立したオブジェクトではなく無限の完全なシーンで1080pの解像度をレンダリングする場合、現在のどの手法もリアルタイム表示（30fps以上）を達成することはできていません。

そこで我々は、競争力のある訓練時間を維持しつつ、最先端の視覚品質を達成し、さらに1080p解像度での高品質なリアルタイム（30fps以上）の新しいビュー合成を可能にする3つの主要な要素を紹介します。第一に、カメラのキャリブレーション中に生成された少量の点から始め、3Dガウシアンでシーンを表現し、シーン最適化のための連続体積放射場の望ましい特性を保持しつつ、空間内の不要な計算を回避します。第二に、3Dガウシアンの密度制御と最適化を交互に行い、特に異方性共分散を最適化することで、シーンの正確な表現を実現します。第三に、異方性スプラッティングをサポートする高速な視界認識レンダリングアルゴリズムを開発し、訓練を加速させるとともにリアルタイムレンダリングを可能にします。我々は、いくつかの確立されたデータセットで最先端の視覚品質とリアルタイムレンダリングを実証します。





intro
メッシュとポイントは、最も一般的な3Dシーンの表現方法です。これらは明示的であり、GPUやCUDAベースの高速なラスタライゼーションに適しています。これに対して、最近のNeural Radiance Field（NeRF）手法は、連続的なシーン表現に基づいており、通常、ボリューメトリックなレイマーチングを使用して新しいビューを合成するために、マルチレイヤーパーセプトロン（MLP）を最適化します。同様に、最も効率的な放射場の解法も連続的な表現を基盤としており、ボクセルやハッシュグリッド、ポイントの値を補間するものです（例：Fridovich-KeilおよびYuら2022、Müllerら2022、Xuら2022）。これらの連続的な性質は最適化を助ける一方で、レンダリングに必要な確率的サンプリングはコストがかかり、ノイズが発生する可能性があります。

私たちは、これらの両方の長所を組み合わせた新しいアプローチを導入します。私たちの3Dガウシアン表現は、最先端の視覚品質（SOTA）と競争力のある訓練時間で最適化を可能にし、タイルベースのスプラッティングソリューションは、複数の既存のデータセットで1080p解像度におけるSOTA品質のリアルタイムレンダリングを保証します。我々の目標は、複数の写真でキャプチャされたシーンのリアルタイムレンダリングを可能にし、最適化時間を過去の効率的な手法と同程度に速くすることです。

最近の手法は高速な訓練を達成していますが（Fridovich-KeilおよびYuら2022、Müllerら2022）、現行の最先端NeRF手法（例：Mip-NeRF360、Barronら2022）ほどの視覚品質を達成するのに苦戦しています。この方法は、訓練に最大48時間を要します。高速ではあるが質の低い放射場手法は、シーンによってインタラクティブなレンダリング時間（10～15フレーム/秒）を達成することができますが、高解像度でのリアルタイムレンダリングには至っていません。

我々のソリューションは、3つの主要な要素に基づいています。まず、柔軟で表現力豊かなシーン表現として3Dガウシアンを導入します。我々は、従来のNeRFに似た手法と同様に、Structure-from-Motion (SfM)【Snavelyら2006】でキャリブレーションされたカメラを使用し、SfMプロセスの一部として無料で生成されるスパースな点群を用いて3Dガウシアンを初期化します。多くのポイントベースの手法がMulti-View Stereo (MVS)データ【Alievら2020; Kopanasら2021; Rückertら2022】を必要とするのに対して、我々はSfMのポイントのみを入力として高品質な結果を達成します。NeRFの合成データセットでは、我々の手法がランダムな初期化でも高品質な結果を出すことを示しています。3Dガウシアンは、微分可能なボリューメトリック表現でありながら、2Dに投影して標準のαブレンディングを適用し、NeRFと同等の画像生成モデルを使用することで効率的にラスタライズできるため、優れた選択肢であることを示します。

次に、3Dガウシアンの3D位置、不透明度α、異方性共分散、球面調和(SH)係数などの特性を最適化し、適応的な密度制御ステップと交互に実行します。この最適化手順では、最適化中に3Dガウシアンを追加したり、時折削除したりします。これにより、コンパクトで非構造化かつ正確なシーン表現（すべてのシーンで1～5百万のガウシアン）を生成します。最後の要素として、最近の研究【LassnerおよびZollhofer 2021】に基づくタイルベースのラスタライゼーションに触発された、高速なGPUソーティングアルゴリズムを使用したリアルタイムレンダリングソリューションを開発しました。我々の3Dガウシアン表現を用いることで、視認性の順序を尊重しつつ、異方性スプラッティングを実行でき、ソートとαブレンディングによって迅速かつ正確なバックワードパスを可能にします。

要約すると、我々の貢献は次の通りです：
- 放射場の高品質で非構造化な表現として、異方性3Dガウシアンを導入しました。
- キャプチャされたシーンの高品質な表現を生成するために、適応的な密度制御と交互に3Dガウシアンの特性を最適化する手法を提案しました。
- GPU向けの高速で微分可能なレンダリング手法を開発し、視認性に対応し、異方性スプラッティングと高速な逆伝播を可能にしました。

我々の結果は、既存のデータセットで3Dガウシアンをマルチビューキャプチャから最適化し、従来の暗黙的放射場手法と同等かそれ以上の品質を達成することを示しています。また、最も高速な手法に匹敵する訓練速度と品質を達成し、特に新しい視点でのリアルタイムレンダリングを高品質で初めて提供します。





related work
まず、伝統的な再構築手法を簡単に概説し、次にポイントベースのレンダリングと放射場に関する研究を取り上げ、その類似点を議論します。放射場は非常に広範な分野であるため、直接関連する研究に焦点を絞ります。この分野の全体像については、最近の優れたレビュー【Tewari et al. 2022; Xie et al. 2022】をご覧ください。

### 2.1 伝統的なシーン再構築とレンダリング
最初の新規ビュー合成手法は、ライトフィールドに基づいており、最初は密にサンプリングされ【Gortler et al. 1996; Levoy and Hanrahan 1996】、次に非構造的なキャプチャを許容しました【Buehler et al. 2001】。Structure-from-Motion (SfM)【Snavely et al. 2006】の登場により、写真のコレクションを使用して新しい視点を合成できる新しい領域が開かれました。SfMはカメラキャリブレーション中にスパースな点群を推定し、これは最初、3D空間のシンプルな可視化に使用されました。続くマルチビュー・ステレオ（MVS）は、数年にわたり印象的な完全3D再構築アルゴリズムを生み出し【Goesele et al. 2007】、いくつかのビュー合成アルゴリズムの開発を可能にしました【Chaurasia et al. 2013; Eisemann et al. 2008; Hedman et al. 2018; Kopanas et al. 2021】。これらの方法はすべて、入力画像を新規ビューカメラに再投影し、ブレンドし、ジオメトリを使って再投影をガイドします。多くの場合、これらの手法は優れた結果をもたらしましたが、未再構築領域や、MVSが存在しないジオメトリを生成してしまう「過剰再構築」から完全に回復することはできませんでした。最近のニューラルレンダリングアルゴリズム【Tewari et al. 2022】は、こうしたアーティファクトを大幅に減らし、すべての入力画像をGPUに保存する圧倒的なコストを回避し、多くの点でこれらの方法を上回っています。

### 2.2 ニューラルレンダリングと放射場

ディープラーニング技術は、早い段階で新規ビュー合成に採用されました【Flynn et al. 2016; Zhou et al. 2016】。例えば、CNNはブレンドウェイトの推定に使用されたり【Hedman et al. 2018】、テクスチャ空間での解決策に利用されました【Riegler and Koltun 2020; Thies et al. 2019】。しかし、これらの多くの方法はMVSベースのジオメトリに依存する点が大きな欠点であり、さらに、最終的なレンダリングにCNNを使用することで、しばしば時間的なフリッカーが発生するという問題もあります。

新規ビュー合成におけるボリュメトリック表現は、Soft3D【Penner and Zhang 2017】によって始まり、続いてボリュメトリックレイマーチングと深層学習技術を組み合わせた手法が提案されました【Henzler et al. 2019; Sitzmann et al. 2019】。これらは、幾何学を表現するための連続的で微分可能な密度場を基盤としています。しかし、ボリュメトリックレイマーチングを使用したレンダリングは、ボリュームを照会するために多数のサンプルが必要であるため、コストがかさむという問題があります。Neural Radiance Fields (NeRFs)【Mildenhall et al. 2020】は、重要なサンプリングと位置エンコーディングを導入し品質を向上させましたが、大規模な多層パーセプトロン（MLP）を使用するため速度が低下しました。

NeRFの成功により、品質と速度に関する問題に取り組む多くの後続研究が爆発的に増加しました。その多くは正則化戦略を導入し、現時点で新規ビュー合成における画像品質の最先端はMip-NeRF360【Barron et al. 2022】です。この方法のレンダリング品質は卓越していますが、トレーニングとレンダリングには非常に長い時間がかかります。私たちの手法は、より速いトレーニングとリアルタイムレンダリングを提供しつつ、品質面でも同等またはそれ以上の成果を上げています。

最近の手法は主にトレーニングやレンダリングの速度向上に焦点を当て、3つの設計選択を活用しています。空間データ構造を使用して（ニューラル）特徴を保存し、それをボリュメトリックレイマーチング中に補間すること、異なるエンコーディングの使用、MLP容量の調整です。これらの方法には、空間の離散化を使用する様々なバリエーション【Chen et al. 2022b,a; Fridovich-Keil and Yu et al. 2022; Garbin et al. 2021; Hedman et al. 2021; Reiser et al. 2021; Takikawa et al. 2021; Wu et al. 2022; Yu et al. 2021】、コードブック【Takikawa et al. 2022】、およびハッシュテーブルのようなエンコーディング【Müller et al. 2022】が含まれており、これにより、より小さなMLPを使用するか、ニューラルネットワークを完全に省略することができます【Fridovich-Keil and Yu et al. 2022; Sun et al. 2022】。

最も注目すべき手法としては、InstantNGP【Müller et al. 2022】があり、これは計算を加速するためにハッシュグリッドと占有グリッドを使用し、密度と外観を表現するためにより小さなMLPを使用しています。また、Plenoxels【Fridovich-Keil and Yu et al. 2022】は、スパースボクセルグリッドを使用して連続的な密度場を補間し、ニューラルネットワークを完全に省略することが可能です。両方の方法は、方向効果を直接表現するために球面調和関数を使用していますが、両方ともシーンやキャプチャタイプによっては空間を効果的に表現するのに苦労することがあります。さらに、選択した構造化グリッドによって、画像品質が制約され、レンダリング速度も制限されます。私たちが使用する非構造化かつGPUに最適化された3Dガウスは、ニューラルコンポーネントを必要とせずに、より速いレンダリング速度とより高い品質を実現しています。

### 2.3 ポイントベースのレンダリングと放射場

ポイントベースの手法は、切り離された非構造化ジオメトリサンプル（ポイントクラウド）を効率的にレンダリングします【Gross and Pfister 2011】。最もシンプルな形態では、ポイントサンプルレンダリング【Grossman and Dally 1998】は、固定サイズの非構造化ポイントのセットをラスタライズします。これには、グラフィックスAPIでネイティブにサポートされているポイントタイプ【Sainz and Pajarola 2004】や、GPU上での並列ソフトウェアラスタライズ【Laine and Karras 2011; Schütz et al. 2022】を活用することができます。しかし、ポイントサンプルレンダリングは元のデータには忠実であるものの、穴が開いたり、エイリアシングが発生したり、連続性が厳密に保たれないという問題があります。高品質のポイントベースのレンダリングに関する先駆的な研究は、ピクセルよりも大きな範囲を持つ円形や楕円形のディスク、楕円体、またはサーフェル【Botsch et al. 2005; Pfister et al. 2000; Ren et al. 2002; Zwicker et al. 2001b】を使用して、これらの問題に対処しています。

近年、微分可能なポイントベースのレンダリング技術への関心が高まっています【Wiles et al. 2020; Yifan et al. 2019】。ポイントはニューラルな特徴で補強され、CNNを使用してレンダリングされることにより、速いまたはリアルタイムのビュー合成が可能になっています【Aliev et al. 2020; Rückert et al. 2022】。しかし、これらの手法は依然として初期のジオメトリにMVSに依存しているため、特に特徴が少ない/光沢のある領域や細い構造など、難しいケースでの過剰再構成や不十分な再構成といったMVS固有のアーティファクトを引き継いでしまいます。

ポイントベースの𝛼ブレンディングとNeRFスタイルのボリュメトリックレンダリングは、本質的に同じ画像生成モデルを共有しています。具体的には、色𝐶は以下のように放射レンダリングにより得られます:

\[
𝐶 = \sum_{i=1}^{N} 𝑇𝑖 (1 − \exp(−𝜎𝑖 𝛿𝑖)) c𝑖
\]
ここで、𝑇𝑖 = \exp\left(-\sum_{j=1}^{i-1} 𝜎𝑗 𝛿𝑗\right)
\]

この式は次のように書き換えることができます:

\[
𝐶 = \sum_{i=1}^{N} 𝑇𝑖 \alpha𝑖 c𝑖
\]
ただし、𝛼𝑖 = (1 − \exp(−𝜎𝑖𝛿𝑖)) であり、𝑇𝑖 = \prod_{j=1}^{i-1}(1 − 𝛼𝑗)です。

典型的なニューラルポイントベースのアプローチ（例:【Kopanas et al. 2022, 2021】）は、ピクセルの色𝐶を、ピクセルに重なるN個の順序付けられたポイントをブレンドすることで計算します:

\[
𝐶 = \sum_{i\in N} c𝑖𝛼𝑖 \prod_{j=1}^{i-1} (1 − 𝛼𝑗)
\]

ここで、c𝑖は各ポイントの色であり、𝛼𝑖は共分散Σを持つ2Dガウス分布の評価結果【Yifan et al. 2019】に学習されたポイントごとの不透明度を掛けたものです。

式 (2) と式 (3) から、画像生成モデルが同一であることが明らかです。しかし、レンダリングアルゴリズムは非常に異なっています。NeRFは、連続的な表現であり、空間の空/満を暗黙的に表現しています。そのため、式 (2) のサンプルを見つけるために高価なランダムサンプリングが必要となり、その結果、ノイズと計算コストが増大します。これに対して、ポイントは非構造化で離散的な表現であり、NeRFのようにジオメトリの生成、消去、配置の移動が可能な柔軟性を持っています。これを実現するためには、不透明度や位置の最適化が必要であり、これは以前の研究【Kopanas et al. 2021】によって示されていますが、完全なボリュメトリック表現の欠点は避けられます。

Pulsar【Lassner and Zollhofer 2021】は、高速な球体ラスタリゼーションを実現し、私たちのタイルベースおよびソーティングされたレンダラーにインスピレーションを与えました。しかし、前述の分析に基づき、我々は（近似的な）従来の𝛼ブレンディングをソートされたスプラットに維持し、ボリュメトリック表現の利点を享受したいと考えています。我々のラスタリゼーションは、順序に依存しないPulsarの手法とは異なり、視界順序を尊重します。さらに、全てのスプラットの勾配をバックプロパゲートし、異方性スプラットをラスタライズします。これらの要素は、我々の結果の高い視覚的品質に寄与しています（セクション7.3参照）。また、前述の方法の多くはレンダリングにCNNを使用しており、これが時間的な不安定性を引き起こします。それにもかかわらず、Pulsar【Lassner and Zollhofer 2021】やADOP【Rückert et al. 2022】のレンダリング速度は、我々の高速レンダリングソリューションの開発動機となりました。

Neural Point Catacaustics【Kopanas et al. 2022】のポイントベースのレンダリングトラックは、鏡面効果に焦点を当てていますが、MLPを使用してこの時間的不安定性を克服しています。しかし、依然としてMVSジオメトリを入力として必要としていました。最近の手法【Zhang et al. 2022】は、MVSを必要とせず、方向のために球面調和関数（SH）を使用していますが、一つのオブジェクトシーンのみを処理でき、初期化にはマスクが必要です。この手法は、小さな解像度と低ポイント数では高速ですが、通常のデータセット【Barron et al. 2022; Hedman et al. 2018; Knapitsch et al. 2017】に対応するためにどのようにスケールできるかは不明です。私たちは、より柔軟なシーン表現のために3Dガウシアンを使用しており、MVSジオメトリを必要とせず、プロジェクションされたガウシアンのタイルベースのレンダリングアルゴリズムのおかげでリアルタイムレンダリングを実現しています。

最近のアプローチ【Xu et al. 2022】は、放射輝度場をラジアル基底関数アプローチでポイントを用いて表現しています。この手法では、最適化中にポイントの剪定や密度化技術を使用していますが、ボリュメトリックレイマーチングを採用しており、リアルタイム表示速度は達成できません。

人間のパフォーマンスキャプチャの分野では、3Dガウシアンが人間の体を表現するために使用されています【Rhodin et al. 2015; Stoll et al. 2011】。最近では、視覚タスクにおいてボリュメトリックレイマーチングとともに使用されています【Wang et al. 2023】。また、類似の文脈では、ニューラルボリュメトリックプリミティブも提案されています【Lombardi et al. 2021】。これらの手法は、私たちがシーン表現として3Dガウシアンを選択する際にインスピレーションを与えましたが、それらは単一の孤立したオブジェクト（人体や顔）の再構築とレンダリングに焦点を当てており、奥行きの複雑さが小さいシーンを対象としています。

これに対して、我々の手法では、異方性共分散の最適化、最適化と密度制御を交互に行う手法、そしてレンダリングにおける効率的な深度ソートを組み合わせることで、屋内外を問わず大きな奥行きの複雑さを含む複雑なシーン全体（背景も含む）を処理することが可能です。





overview
私たちの手法への入力は、静的シーンの画像セットと、SfM（Structure-from-Motion）によってキャリブレーションされた対応するカメラのセットです【Schönberger and Frahm 2016】。SfMの副産物として得られるスパースポイントクラウドから、位置（平均）、共分散行列、及び opacity 𝛼 で定義される3Dガウシアンのセットを作成します（Sec. 4）。これにより非常に柔軟な最適化を行うことができ、3Dシーンの比較的コンパクトな表現を得ることができます。特に、高度に異方的なボリュメトリックスプラットを使用することで、細かい構造をコンパクトに表現することが可能です。

放射輝度場の方向性外観成分（色）は、標準的な手法に従い球面調和（SH）を用いて表現されます【Fridovich-Keil and Yu et al. 2022; Müller et al. 2022】。私たちのアルゴリズムは、3Dガウシアンのパラメータ（位置、共分散、𝛼、SH係数）を最適化する一連のステップを経て放射輝度場表現を作成します（Sec. 5）。この際、ガウシアンの密度を適応的に制御する操作を交互に行います。

私たちの手法の効率性の鍵は、タイルベースのラスタライザー（Sec. 6）にあります。このラスタライザーは、異方的スプラットの𝛼ブレンディングを行い、迅速なソートによって可視性の順序を尊重します。また、迅速なバックワードパスも含まれており、累積した𝛼値を追跡することで、勾配を受け取るガウシアンの数に制限はありません。私たちの手法の概要は、Fig. 2に示されています。





4 differentiavle 3d gaussian splatting
私たちの目標は、高品質の新規視点合成を可能にするシーン表現を最適化することであり、スパースなSfMポイントから始めることです。これを実現するためには、微分可能なボリュメトリック表現の特性を引き継ぎつつ、非常に高速なレンダリングを可能にするために非構造的かつ明示的なプリミティブが必要です。そこで、私たちは3Dガウシアンを選択しました。これは微分可能であり、2Dスプラットに簡単に投影できるため、レンダリングにおける高速な𝛼ブレンディングが可能です。

私たちの表現は、2Dポイントを使用する以前の方法【Kopanas et al. 2021; Yifan et al. 2019】と類似しており、各ポイントが法線を持つ小さな平面円であると仮定しています。しかし、SfMポイントの極端なスパース性により、法線を推定することは非常に困難です。同様に、そのような推定から非常にノイジーな法線を最適化することも困難です。そこで、私たちは、法線を必要としない3Dガウシアンのセットとして幾何学をモデル化します。私たちのガウシアンは、ワールドスペースで定義された完全な3D共分散行列Σで定義され、点（平均）𝜇を中心に持っています。

\[
G(x) = e^{-\frac{1}{2}(x)^T Σ^{-1}(x)}
\]

このガウシアンは、ブレンディングプロセスで𝛼と掛け合わされます。ただし、レンダリングのために3Dガウシアンを2Dに投影する必要があります。Zwickerら【2001a】は、この投影方法を画像空間に示しています。視点変換𝑊を考慮すると、カメラ座標系における共分散行列Σ'は次のように与えられます：

\[
Σ' = J W Σ W^T J^T
\]

ここで、𝐽は射影変換のアフィン近似のヤコビアンです。また、Zwickerらは、Σ'の第三行と第三列をスキップすることで、法線を持つ平面ポイントから始めた場合と同じ構造と特性を持つ2×2の分散行列を得られることを示しています【Kopanas et al. 2021】。

共分散行列Σを直接最適化して放射輝度場を表現する3Dガウシアンを得る明らかなアプローチがありますが、共分散行列は正定値半であるときに物理的意味を持ちます。私たちのパラメータ全体の最適化には勾配降下法を使用していますが、これはこのような有効な行列を生成するように簡単には制約できず、更新ステップや勾配は非常に簡単に無効な共分散行列を生成してしまいます。

そのため、私たちは最適化のためにより直感的でありながら同等に表現力のある表現を選択しました。3Dガウシアンの共分散行列Σは、楕円体の構成を記述することに類似しています。スケーリング行列𝑆と回転行列𝑅を考慮すると、対応するΣを次のように求めることができます：

\[
Σ = R S S^T R^T
\]

両方の要素を独立して最適化できるように、これらを別々に保存します：スケーリングのための3Dベクトル𝑠と、回転を表すクォータニオン𝑞です。これらはそれぞれの行列に簡単に変換でき、結合することで有効な単位クォータニオンを得るために𝑞を正規化します。

トレーニング中の自動微分による重大なオーバーヘッドを避けるために、すべてのパラメータの勾配を明示的に導出します。具体的な導関数計算の詳細は付録Aに記載されています。このような最適化に適した異方性共分散の表現により、3Dガウシアンを最適化してキャプチャされたシーンの異なる形状に適応させることができ、比較的コンパクトな表現を実現しています。Fig. 3はそのようなケースを示しています。





5 optimization with adaptive density control of 3d gaussians
私たちのアプローチの核心は、シーンを正確に表現する密な3Dガウシアンのセットを生成する最適化ステップです。このプロセスでは、位置 \( p \)、不透明度 \( \alpha \)、および共分散 \( \Sigma \) に加えて、各ガウシアンの色 \( c \) を表す球面調和（SH）係数も最適化し、シーンの視点依存の外観を正確にキャプチャします。これらのパラメータの最適化は、シーンをより良く表現するためにガウシアンの密度を制御するステップと交互に行われます。

### 5.1 最適化

最適化は、レンダリングの反復と、その結果得られた画像とキャプチャされたデータセットのトレーニングビューとの比較に基づいています。3Dから2Dへの投影の曖昧さのために、ジオメトリが誤って配置されることは避けられません。したがって、最適化は、誤って配置されたジオメトリを作成するだけでなく、破壊または移動する能力も必要です。3Dガウシアンの共分散のパラメータの品質は、表現のコンパクトさにとって重要であり、広い均質領域は、少数の大きな異方性ガウシアンでキャプチャできます。

最適化には確率的勾配降下法（SGD）を使用し、標準のGPU加速フレームワークを最大限に活用し、一部の操作に対してカスタムCUDAカーネルを追加することができるようにします。これは、最近のベストプラクティスに従ったものです【Fridovich-Keil and Yu et al. 2022; Sun et al. 2022】。特に、私たちの高速ラスタリゼーション（セクション6参照）は、最適化の効率において重要であり、最適化の主な計算ボトルネックとなります。

不透明度 \( \alpha \) にはシグモイド活性化関数を使用して、値を [0, 1) の範囲に制約し、滑らかな勾配を得るとともに、共分散のスケールには指数関数的活性化関数を使用して同様の理由を持ちます。初期共分散行列は、最も近い3つの点への距離の平均に等しい軸を持つ等方性ガウシアンとして推定します。

私たちは、Plenoxels【Fridovich-Keil and Yu et al. 2022】に似た標準的な指数減衰スケジューリング技術を使用しますが、位置にのみ適用します。損失関数は、L1とD-SSIM項の組み合わせで定義されます：

\[
L = (1 - \lambda)L_1 + \lambda L_{D-SSIM} \quad (7)
\]

すべてのテストで \( \lambda = 0.2 \) を使用します。学習スケジュールやその他の要素の詳細は、セクション7.1で提供します。

### 5.2 ガウシアンの適応制御

最初に、SfMから得られた初期のスパースポイントセットを使用し、次に、単位体積あたりのガウシアンの数と密度を適応的に制御する手法を適用します。これにより、初期のスパースガウシアンセットから、シーンをより適切に表現し、正しいパラメータを持つより密なセットへと進化させることができます。最適化のウォームアップ後（セクション7.1参照）、私たちは100イテレーションごとに密度を増加させ、透明度が閾値 \( \epsilon_\alpha \) よりも小さいガウシアンを削除します。

ガウシアンの適応的制御は、空白領域を埋める必要があります。これは、幾何学的特徴が不足している（「アンダーリコンストラクション」）領域や、シーンの大きな面積をカバーするガウシアン（しばしば「オーバーリコンストラクション」に相当）にも焦点を当てます。両者には大きなビュー空間の位置勾配が存在することが観察されます。直感的には、これはこれらの領域がまだ十分に再構築されていないことに起因し、最適化がこれを修正するためにガウシアンを移動させようとしているためです。

両方のケースが密度増加の良い候補であるため、私たちは位置勾配の平均の大きさが閾値 \( \tau_{\text{pos}} \) を超えるガウシアンを密度増加させます。実験的に、この閾値を0.0002に設定しました。このプロセスの詳細は、図4に示されています。

アンダーリコンストラクション領域にある小さなガウシアンについては、新しいジオメトリをカバーする必要があります。これには、同じサイズのガウシアンのコピーを作成し、その位置を勾配の方向に移動させることで複製するのが好ましいです。一方、高い分散がある領域にある大きなガウシアンは、より小さなガウシアンに分割する必要があります。このようなガウシアンは、2つの新しいガウシアンに置き換え、実験的に決定した \( \phi = 1.6 \) の因子でスケールを分割します。また、元の3DガウシアンをPDFとしてサンプリングして位置を初期化します。

最初のケースでは、システムの総体積とガウシアンの数の両方を増加させる必要があることを検出し、処理します。一方、2番目のケースでは、総体積は保存しながらガウシアンの数を増やします。その他のボリュメトリック表現と同様に、私たちの最適化は、入力カメラに近いフローティングオブジェクトで詰まる可能性があります。この場合、ガウシアン密度の不当な増加が生じる可能性があります。ガウシアンの数の増加を調整する効果的な方法は、3000イテレーションごとに \( \alpha \) 値をゼロ近くに設定することです。最適化は、必要なガウシアンの \( \alpha \) を増加させる一方で、上記で説明したように \( \epsilon_\alpha \) よりも小さい \( \alpha \) を持つガウシアンを削除することを許可します。ガウシアンは縮小または成長し、他のガウシアンと大きく重なることがありますが、非常に大きなワールドスペースのガウシアンやビュー空間における大きなフットプリントを持つガウシアンは定期的に削除します。この戦略により、ガウシアンの総数を全体的に良好に制御できます。私たちのモデル内のガウシアンは常にユークリッド空間の原始的なものであり、他の手法【Barron et al. 2022; Fridovich-Keil and Yu et al. 2022】とは異なり、遠方または大きなガウシアンのための空間圧縮、変形、または投影戦略を必要としません。





6 fast diffrenriable rasterizer for gaussians
### 6.1 タイルベースのラスタライザー

私たちの目標は、全体的なレンダリングの速度とソートの速度を向上させ、アニソトロピック・スプラットに対する近似的な \( \alpha \)-ブレンディングを実現し、以前の研究【Lassner and Zollhofer 2021】で存在した勾配を受け取るスプラットの数に対する厳しい制限を回避することです。この目標を達成するために、最近のソフトウェアラスタライゼーション手法【Lassner and Zollhofer 2021】からインスパイアを受けたガウシアン・スプラットのためのタイルベースのラスタライザーを設計しました。このラスタライザーは、全画像に対してプリソートを行うことで、以前の \( \alpha \)-ブレンディングソリューション【Kopanas et al. 2022, 2021】が妨げられたピクセルごとのソートのコストを回避します。

私たちの高速ラスタライザーは、追加のメモリ消費が少なく、低いオーバーヘッドで任意の数のブレンドされたガウシアンに対して効率的な逆伝播を可能にします。私たちのラスタライゼーションパイプラインは完全に微分可能であり、2Dへの投影（セクション4参照）を考慮すると、以前の2Dスプラッティング手法【Kopanas et al. 2021】と同様にアニソトロピックスプラットをラスタライズできます。

この方法では、画面を16×16のタイルに分割し、ビュー・フラスタムおよび各タイルに対して3Dガウシアンをカリングします。具体的には、99%の信頼区間がビュー・フラスタムと交差しているガウシアンのみを保持します。さらに、極端な位置にあるガウシアン（すなわち、近接平面に近く、ビュー・フラスタムの外にある平均値を持つもの）をトリビアルに拒否するためにガードバンドを使用します。これにより、彼らの投影された2D共分散を計算する際の不安定さを回避します。

次に、オーバーラップするタイルの数に応じて各ガウシアンをインスタンス化し、ビュー空間の深さとタイルIDを組み合わせたキーを各インスタンスに割り当てます。次に、GPUの高速ラディックスソート【Merrill and Grimshaw 2010】を使用して、これらのキーに基づいてガウシアンをソートします。注意すべき点は、追加のピクセルごとのポイントの順序付けは行われず、ブレンディングはこの初期ソートに基づいて実行されることです。その結果、私たちの \( \alpha \)-ブレンディングは、いくつかの設定において近似的なものとなる可能性があります。しかし、スプラットが個々のピクセルのサイズに近づくにつれて、これらの近似は無視できるものになります。この選択は、収束したシーンに目に見えるアーティファクトを生じさせることなく、トレーニングとレンダリングのパフォーマンスを大幅に向上させることがわかりました。

### 6.2 ラスタライゼーションの詳細

ガウシアンのソート後、各タイルに対して、特定のタイルにスプラットする最初および最後の深さソートされたエントリを特定することでリストを生成します。ラスタライゼーションのために、各タイルごとに1つのスレッドブロックを起動します。各ブロックは最初に協調的にガウシアンのパケットを共有メモリにロードし、その後、特定のピクセルに対してリストを前から後ろへと走査し、色と \( \alpha \) 値を累積します。これにより、データの読み込み/共有と処理の両方において並列性の向上が最大化されます。ピクセルでの \( \alpha \) の目標飽和に達した時点で、対応するスレッドは停止します。定期的に、タイル内のスレッドにクエリを実行し、全てのピクセルが飽和（つまり \( \alpha \) が1になる）すると、タイル全体の処理が終了します。ソートの詳細と全体のラスタライゼーションアプローチの高レベルの概要は、付録Cに記載されています。

ラスタライゼーション中、\( \alpha \) の飽和が唯一の停止基準となります。以前の研究とは異なり、勾配の更新を受けるブレンドされたプリミティブの数に制限を設けません。このプロパティを強制することで、任意の深度複雑性を持つシーンに対処し、正確に学習できるようになります。また、シーン固有のハイパーパラメータ調整に頼る必要がありません。

バックワードパス中は、フォワードパスでのブレンドされたポイントの完全なシーケンスをピクセルごとに復元する必要があります。一つの解決策は、グローバルメモリにピクセルごとのブレンドポイントの任意に長いリストを保存することです【Kopanas et al. 2021】。暗示される動的メモリ管理のオーバーヘッドを避けるために、私たちは再びパーセルリストを走査することを選択します。フォワードパスからのソートされたガウシアンの配列とタイル範囲を再利用できます。勾配計算を容易にするために、今回はバックトゥフロントで走査します。

走査は、タイル内の任意のピクセルに影響を与えた最後のポイントから始まります。また、ポイントの共有メモリへのロードは再び協調的に行われます。さらに、各ピクセルは、その深度がフォワードパス中に色に寄与した最後のポイントの深度以下である場合にのみ、（コストのかかる）オーバーラップテストおよびポイントの処理を開始します。セクション4で説明した勾配計算は、元のブレンディングプロセス中の各ステップでの累積オパシティ値を必要とします。バックワードパスで進行中のオパシティのリストを走査するのではなく、フォワードパスの最後に総累積オパシティのみを保存することで、これらの中間オパシティを復元できます。具体的には、各ポイントはフォワードプロセス中の最終累積オパシティ \( \alpha \) を保存します。バックトゥフロントの走査中に、これを各ポイントの \( \alpha \) で割ることで、勾配計算に必要な係数を取得します。





7 implemettation results evaluation
### 7. 実装および評価

#### 7.1 実装
私たちの手法は、PythonとPyTorchフレームワークを使用して実装されており、ラスタライゼーション用にカスタムCUDAカーネルを作成しています。これらは、以前の手法【Kopanas et al. 2021】の拡張版であり、迅速な基数ソートのためにNVIDIAのCUBソーティングルーチンを使用しています。また、インタラクティブビューアも開発し、オープンソースのSIBR【Bonopera et al. 2020】を使用してインタラクティブな表示を行っています。この実装を用いて、達成したフレームレートを測定しました。ソースコードおよびすべてのデータは、次のURLで入手可能です: [https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)

#### 最適化の詳細
安定性を確保するために、計算を低解像度で「ウォームアップ」します。具体的には、最適化を4倍小さい画像解像度から開始し、250回および500回のイテレーション後に2回アップサンプリングを行います。

SH係数の最適化は、角度情報の欠如に敏感です。典型的な「NeRFライク」キャプチャでは、中央のオブジェクトがその周りの全ての半球から撮影された写真によって観察されるため、最適化はうまく機能します。しかし、キャプチャに角度領域が欠如している場合（例えば、シーンの隅をキャプチャする場合や、「内向き」キャプチャ【Hedman et al. 2016】を行う場合）、最適化によってSHのゼロ階成分（基底または拡散色）の完全に不正確な値が生成されることがあります。この問題を克服するために、最初にゼロ階成分のみを最適化し、その後、1000イテレーションごとにSHのバンドを1つずつ追加し、最終的には全ての4つのSHバンドが表現されるようにしています。

### 7.2 結果と評価

#### 結果
私たちのアルゴリズムを、以前に公開されたデータセットから取得した13の実世界のシーンと、合成Blenderデータセット【Mildenhall et al. 2020】でテストしました。特に、現在のNeRFレンダリング品質の最先端であるMip-Nerf360【Barron et al. 2022】に提示されているシーンのフルセット、Tanks&Templesデータセット【2017】からの2シーン、およびHedman et al.【Hedman et al. 2018】が提供した2シーンを対象としました。選択したシーンは非常に異なるキャプチャスタイルを持ち、境界のある屋内シーンと大規模な境界のない屋外環境の両方をカバーしています。すべての評価実験では同じハイパーパラメータ設定を使用しました。すべての結果はA6000 GPU上で実行されており、Mip-NeRF360メソッドについては例外があります（詳細は下記）。

補足資料では、入力写真から遠い視点を含むシーンのレンダリングされた動画パスを示しています。

#### 実世界のシーン
品質に関して、現在の最先端はMip-Nerf360【Barron et al. 2021】です。このメソッドと比較して品質ベンチマークを行います。また、最近の高速NeRFメソッドであるInstantNGP【Müller et al. 2022】とPlenoxels【Fridovich-Keil and Yu et al. 2022】とも比較します。

データセットのトレイン/テストスプリットを行い、Mip-Nerf360が提案した方法論を使用して、テスト用に8枚ごとに1枚の写真を取得し、一貫した意味のある比較を行って誤差メトリックを生成しました。使用する標準的なメトリックにはPSNR、L-PIPS、SSIMが含まれ、これらは文献で最も頻繁に使用されています。詳細は表1を参照してください。表内の数値はすべて私たち自身の実行によるものであり、Mip-Nerf360データセットの数値は元の出版物からコピーしたもので、現在のSOTAに関する混乱を避けるためです。図中の画像に関しては、私たちのMip-Nerf360の実行結果を使用しています。これらの実行の数値はAppendix Dに示されています。さらに、平均トレーニング時間、レンダリング速度、および最適化されたパラメータを格納するために使用されるメモリも示します。InstantNGPの基本設定（Base）で35Kイテレーションを実行した結果と、著者が提案したやや大きなネットワーク（Big）、および私たちの設定である7Kと30Kイテレーションの結果を報告します。私たちの2つの設定の視覚品質の違いを図6に示します。多くの場合、7Kイテレーションでの品質はすでに非常に良好です。

### トレーニング時間と結果

トレーニング時間はデータセットによって異なるため、個別に報告します。データセットによって画像解像度も異なることに注意してください。プロジェクトのウェブサイトでは、すべてのメソッド（私たちのものと以前の作業）の統計を計算するために使用したテストビューのすべてのレンダリングを提供しています。すべてのレンダリングでは、元の入力解像度を維持しました。

表に示すように、私たちの完全に収束したモデルは、SOTAのMip-NeRF360メソッドと同等の品質を達成しており、時にはそれを少し上回ることもあります。なお、同じハードウェア上での彼らの平均トレーニング時間は48時間であるのに対し、私たちは35～45分で済んでおり、彼らのレンダリング時間は1フレームあたり10秒です。私たちは、InstantNGPおよびPlenoxelsと同等の品質を5～10分のトレーニングで達成しましたが、追加のトレーニング時間により、他の高速メソッドでは達成できないSOTA品質を実現できます。Tanks & Templesでは、基本的なInstantNGPと同様のトレーニング時間（私たちの場合は約7分）で同等の品質を達成しています。

比較のために選択した以前のレンダリング手法との左外れテストビューの視覚結果を図5に示します。私たちの結果は30Kイテレーションのトレーニングに基づいています。いくつかのケースでは、Mip-NeRF360には私たちの手法が回避する残存アーティファクト（例えば、自転車や切り株の植物のぼやけや、部屋の壁のぼやけ）が見受けられます。補足のビデオとウェブページでは、距離からの経路の比較を提供しています。私たちの手法は、遠くからでもしっかりと覆われた領域の視覚的な詳細を保持する傾向がありますが、これは以前の手法では常にそうであるとは限りません。

### 合成境界シーン
現実的なシーンに加えて、合成Blenderデータセット【Mildenhall et al. 2020】でもアプローチを評価しました。問題のシーンは、網羅的なビューセットを提供し、サイズが制限されており、正確なカメラパラメータを提供します。このようなシナリオでは、ランダム初期化でも最先端の結果を達成できます。シーンの境界を囲むボリューム内に100Kの均一にランダムなガウスを持ってトレーニングを開始します。私たちのアプローチはそれらを迅速に自動的に約6～10Kの意味のあるガウスにプルーニングします。30Kイテレーション後のトレーニング済みモデルの最終サイズは、シーンごとに約200～500Kのガウスに達します。以前のメソッドとの比較において、達成したPSNRスコアを表2に示します。互換性のために白い背景を使用しています。例は図10（左から2番目の画像）および補足資料に示されています。トレーニング済みの合成シーンは、180～300 FPSでレンダリングされました。

### コンパクトさ

以前の明示的なシーン表現と比較して、私たちの最適化で使用される非等方ガウスは、より少ないパラメータで複雑な形状をモデル化する能力があります。私たちは、[Zhang et al. 2022]によって得られた非常にコンパクトなポイントベースのモデルと比較することでこれを示します。

最初に、前景マスクを用いて空間キャービングから得られた彼らの初期ポイントクラウドから始め、報告されたPSNRスコアと同等になるまで最適化します。これは通常、2〜4分以内に達成されます。私たちは、彼らのポイント数の約4分の1で報告されたメトリクスを上回り、平均モデルサイズは3.8MBで、彼らの9MBに対して優れた結果を示しています。この実験では、彼らと同様に球面調和の2次のみを使用したことに注意してください。

### 7.3 アブレーション

私たちは、異なる貢献やアルゴリズムの選択を分離し、それらの効果を測定するための一連の実験を構築しました。具体的には、以下のアルゴリズムの側面をテストします。

- SfMからの初期化
- 密度化戦略
- 非等方共分散
- 限制のないスプラットに勾配を許可すること
- 球面調和の使用

各選択肢の定量的な効果は、表3に要約されています。

#### SfMからの初期化
3DガウスをSfMポイントクラウドから初期化することの重要性も評価します。このアブレーションでは、入力カメラのバウンディングボックスの範囲の3倍のサイズを持つ立方体を均等にサンプリングします。SfMポイントがなくても私たちの方法は比較的良好に機能し、完全な失敗を回避します。しかし、主に背景での性能が低下します（図7参照）。また、トレーニングビューで十分にカバーされていない領域では、ランダム初期化法には、最適化によって除去できない「フロート」現象が多く見られます。一方、合成NeRFデータセットには背景がなく、入力カメラによってしっかりと制約されているため、このような挙動は見られません（上記の議論を参照）。

#### 密度化
次に、セクション5で説明したクローンおよびスプリット戦略という2つの密度化方法を評価します。それぞれの方法を無効にし、他の部分は変更せずに最適化します。結果は、大きなガウスを分割することが背景の良好な再構築を可能にすることが重要であることを示しています（図8参照）。一方で、小さなガウスを分割するのではなくクローンすることは、シーンに細い構造が現れた場合に、より良くかつ速い収束を可能にします。

### 無制限の深さ複雑性を持つスプラットに対する勾配の評価

私たちは、フロント最前面の \(N\) ポイントの後に勾配計算をスキップすることで、品質を損なうことなく速度を向上できるかを評価します。これは、Pulsar [Lassner and Zollhofer 2021] において提案されています。このテストでは、\(N=10\) を選択しました。これは、Pulsar のデフォルト値の2倍ですが、勾配計算の厳しい近似により最適化が不安定になりました。具体的には、トラックシーンでは、PSNRが11dB低下しました（表3、Limited-BW参照）。視覚的な結果は図9に示されています（ガーデンの例）。

### 非等方共分散
私たちの手法における重要なアルゴリズムの選択は、3Dガウスのフル共分散行列の最適化です。この選択の影響を示すために、全ての軸で3Dガウスの半径を制御する単一のスカラー値を最適化することで非等方性を排除したアブレーションを実施しました。この最適化の結果は、図10に視覚的に示されています。非等方性を考慮することで、3Dガウスが表面に合わせる能力が大幅に向上し、それによって同じポイント数を維持しながらも、はるかに高いレンダリング品質が実現されることが観察されました。

### 球面調和
最後に、球面調和を使用することで、視点依存の効果を補うことができるため、全体的なPSNRスコアが改善されます（表3参照）。

### 7.4 制限事項

私たちの手法には限界が存在します。シーンが十分に観測されていない領域ではアーティファクトが発生し、そのような領域では他の手法も苦しむ傾向があります（例：図11のMip-NeRF360）。上記で述べたように、非等方ガウスには多くの利点がありますが、細長いアーティファクトや「スポッティ」なガウスが生成されることがあります（図12参照）。これもまた、以前の手法が同様の問題を抱えています。

また、最適化中に大きなガウスが生成されると、時折ポッピングアーティファクトが発生します。これは主に、ラスタライザーでのガードバンドによるガウスの単純な拒否が原因です。より原則的なカリングアプローチを採用すれば、これらのアーティファクトを軽減できるでしょう。もう一つの要因は、簡単な可視性アルゴリズムであり、これが原因でガウスが突然深度やブレンディング順序を変更することがあります。これはアンチエイリアス処理によって対処可能ですが、今後の研究課題としています。

現在のところ、私たちの最適化に対して正則化を適用しておらず、これを行うことで未観測領域やポッピングアーティファクトの問題が改善される可能性があります。全体の評価に同じハイパーパラメータを使用しましたが、初期の実験では、非常に大きなシーン（例：都市データセット）で収束させるために位置学習率を下げる必要があることが示されました。

私たちのアプローチは、以前の点ベースの手法と比較して非常にコンパクトであるものの、NeRFベースのソリューションに比べてメモリ消費が大幅に高くなります。大規模なシーンのトレーニング中、ピークGPUメモリ消費量は最適化されていないプロトタイプで20GBを超えることがあります。しかし、この数値は、InstantNGPのように最適化ロジックの低レベルの実装を慎重に行うことで大幅に削減できる可能性があります。トレーニングされたシーンをレンダリングするには、フルモデルを保存するために十分なGPUメモリが必要で（大規模シーンでは数百MB）、シーンサイズや画像解像度に応じてラスタライザーに追加で30～500MBが必要です。私たちの手法のメモリ消費をさらに減少させる機会は多くあります。ポイントクラウドの圧縮技術はよく研究されている分野であり[De Queiroz and Chou 2016]、このようなアプローチが私たちの表現にどのように適用できるかを調べることは興味深いでしょう。





8 discussion and conclusions
私たちは、さまざまなシーンやキャプチャスタイルにおいて、リアルタイムで高品質な放射フィールドレンダリングを実現する初めてのアプローチを提示しました。これは、以前の最速手法と競争力のあるトレーニング時間を必要とします。

3Dガウス原始体を選択することで、最適化のためのボリュメトリックレンダリングの特性を保持しつつ、迅速なスプラットベースのラスタライゼーションを直接可能にしています。私たちの研究は、広く受け入れられている意見に反して、連続的な表現は高速かつ高品質な放射フィールドのトレーニングを実現するために厳密には必要ないことを示しています。

トレーニング時間の約80%はPythonコードに費やされており、私たちのソリューションはPyTorchで構築されているため、他の人々が簡単に利用できるようになっています。ラスタライゼーションルーチンのみが最適化されたCUDAカーネルとして実装されています。InstantNGP [Müller et al. 2022]で行われたように、残りの最適化を完全にCUDAに移植することで、パフォーマンスが重要なアプリケーションにおいて大幅なスピードアップが可能になると期待しています。

また、リアルタイムレンダリングの原則に基づく重要性を示し、GPUのパワーとソフトウェアラスタライゼーションパイプラインアーキテクチャの速度を活用しています。これらの設計選択は、トレーニングとリアルタイムレンダリングの両方において性能の鍵となり、以前のボリュメトリックレイマーチングに対する競争力を提供します。

私たちのガウスがキャプチャされたシーンのメッシュ再構築に利用できるかどうかは興味深い課題です。メッシュの広範な使用を考慮すると、これは実用的な含意があり、私たちの手法がボリュメトリック表現と表面表現の連続体の中でどの位置にあるのかをよりよく理解する手助けとなるでしょう。

結論として、私たちは放射フィールドのための初のリアルタイムレンダリングソリューションを提示しました。このソリューションは、最も高価な以前の手法と同等のレンダリング品質を持ち、最速の既存ソリューションと競争力のあるトレーニング時間を必要とします。