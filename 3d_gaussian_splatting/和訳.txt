最初のところ
Radiance Field（放射場）手法は、複数の写真やビデオで撮影されたシーンの新しいビュー合成において、最近革新的な進展を遂げています。しかし、高い視覚品質を達成するためには、訓練やレンダリングに高コストなニューラルネットワークが必要であり、最近の高速化手法では、必然的に速度と品質のトレードオフが生じます。特に、孤立したオブジェクトではなく無限の完全なシーンで1080pの解像度をレンダリングする場合、現在のどの手法もリアルタイム表示（30fps以上）を達成することはできていません。

そこで我々は、競争力のある訓練時間を維持しつつ、最先端の視覚品質を達成し、さらに1080p解像度での高品質なリアルタイム（30fps以上）の新しいビュー合成を可能にする3つの主要な要素を紹介します。第一に、カメラのキャリブレーション中に生成された少量の点から始め、3Dガウシアンでシーンを表現し、シーン最適化のための連続体積放射場の望ましい特性を保持しつつ、空間内の不要な計算を回避します。第二に、3Dガウシアンの密度制御と最適化を交互に行い、特に異方性共分散を最適化することで、シーンの正確な表現を実現します。第三に、異方性スプラッティングをサポートする高速な視界認識レンダリングアルゴリズムを開発し、訓練を加速させるとともにリアルタイムレンダリングを可能にします。我々は、いくつかの確立されたデータセットで最先端の視覚品質とリアルタイムレンダリングを実証します。



intro
メッシュとポイントは、最も一般的な3Dシーンの表現方法です。これらは明示的であり、GPUやCUDAベースの高速なラスタライゼーションに適しています。これに対して、最近のNeural Radiance Field（NeRF）手法は、連続的なシーン表現に基づいており、通常、ボリューメトリックなレイマーチングを使用して新しいビューを合成するために、マルチレイヤーパーセプトロン（MLP）を最適化します。同様に、最も効率的な放射場の解法も連続的な表現を基盤としており、ボクセルやハッシュグリッド、ポイントの値を補間するものです（例：Fridovich-KeilおよびYuら2022、Müllerら2022、Xuら2022）。これらの連続的な性質は最適化を助ける一方で、レンダリングに必要な確率的サンプリングはコストがかかり、ノイズが発生する可能性があります。

私たちは、これらの両方の長所を組み合わせた新しいアプローチを導入します。私たちの3Dガウシアン表現は、最先端の視覚品質（SOTA）と競争力のある訓練時間で最適化を可能にし、タイルベースのスプラッティングソリューションは、複数の既存のデータセットで1080p解像度におけるSOTA品質のリアルタイムレンダリングを保証します。我々の目標は、複数の写真でキャプチャされたシーンのリアルタイムレンダリングを可能にし、最適化時間を過去の効率的な手法と同程度に速くすることです。

最近の手法は高速な訓練を達成していますが（Fridovich-KeilおよびYuら2022、Müllerら2022）、現行の最先端NeRF手法（例：Mip-NeRF360、Barronら2022）ほどの視覚品質を達成するのに苦戦しています。この方法は、訓練に最大48時間を要します。高速ではあるが質の低い放射場手法は、シーンによってインタラクティブなレンダリング時間（10～15フレーム/秒）を達成することができますが、高解像度でのリアルタイムレンダリングには至っていません。

我々のソリューションは、3つの主要な要素に基づいています。まず、柔軟で表現力豊かなシーン表現として3Dガウシアンを導入します。我々は、従来のNeRFに似た手法と同様に、Structure-from-Motion (SfM)【Snavelyら2006】でキャリブレーションされたカメラを使用し、SfMプロセスの一部として無料で生成されるスパースな点群を用いて3Dガウシアンを初期化します。多くのポイントベースの手法がMulti-View Stereo (MVS)データ【Alievら2020; Kopanasら2021; Rückertら2022】を必要とするのに対して、我々はSfMのポイントのみを入力として高品質な結果を達成します。NeRFの合成データセットでは、我々の手法がランダムな初期化でも高品質な結果を出すことを示しています。3Dガウシアンは、微分可能なボリューメトリック表現でありながら、2Dに投影して標準のαブレンディングを適用し、NeRFと同等の画像生成モデルを使用することで効率的にラスタライズできるため、優れた選択肢であることを示します。

次に、3Dガウシアンの3D位置、不透明度α、異方性共分散、球面調和(SH)係数などの特性を最適化し、適応的な密度制御ステップと交互に実行します。この最適化手順では、最適化中に3Dガウシアンを追加したり、時折削除したりします。これにより、コンパクトで非構造化かつ正確なシーン表現（すべてのシーンで1～5百万のガウシアン）を生成します。最後の要素として、最近の研究【LassnerおよびZollhofer 2021】に基づくタイルベースのラスタライゼーションに触発された、高速なGPUソーティングアルゴリズムを使用したリアルタイムレンダリングソリューションを開発しました。我々の3Dガウシアン表現を用いることで、視認性の順序を尊重しつつ、異方性スプラッティングを実行でき、ソートとαブレンディングによって迅速かつ正確なバックワードパスを可能にします。

要約すると、我々の貢献は次の通りです：
- 放射場の高品質で非構造化な表現として、異方性3Dガウシアンを導入しました。
- キャプチャされたシーンの高品質な表現を生成するために、適応的な密度制御と交互に3Dガウシアンの特性を最適化する手法を提案しました。
- GPU向けの高速で微分可能なレンダリング手法を開発し、視認性に対応し、異方性スプラッティングと高速な逆伝播を可能にしました。

我々の結果は、既存のデータセットで3Dガウシアンをマルチビューキャプチャから最適化し、従来の暗黙的放射場手法と同等かそれ以上の品質を達成することを示しています。また、最も高速な手法に匹敵する訓練速度と品質を達成し、特に新しい視点でのリアルタイムレンダリングを高品質で初めて提供します。



related work
まず、伝統的な再構築手法を簡単に概説し、次にポイントベースのレンダリングと放射場に関する研究を取り上げ、その類似点を議論します。放射場は非常に広範な分野であるため、直接関連する研究に焦点を絞ります。この分野の全体像については、最近の優れたレビュー【Tewari et al. 2022; Xie et al. 2022】をご覧ください。

### 2.1 伝統的なシーン再構築とレンダリング
最初の新規ビュー合成手法は、ライトフィールドに基づいており、最初は密にサンプリングされ【Gortler et al. 1996; Levoy and Hanrahan 1996】、次に非構造的なキャプチャを許容しました【Buehler et al. 2001】。Structure-from-Motion (SfM)【Snavely et al. 2006】の登場により、写真のコレクションを使用して新しい視点を合成できる新しい領域が開かれました。SfMはカメラキャリブレーション中にスパースな点群を推定し、これは最初、3D空間のシンプルな可視化に使用されました。続くマルチビュー・ステレオ（MVS）は、数年にわたり印象的な完全3D再構築アルゴリズムを生み出し【Goesele et al. 2007】、いくつかのビュー合成アルゴリズムの開発を可能にしました【Chaurasia et al. 2013; Eisemann et al. 2008; Hedman et al. 2018; Kopanas et al. 2021】。これらの方法はすべて、入力画像を新規ビューカメラに再投影し、ブレンドし、ジオメトリを使って再投影をガイドします。多くの場合、これらの手法は優れた結果をもたらしましたが、未再構築領域や、MVSが存在しないジオメトリを生成してしまう「過剰再構築」から完全に回復することはできませんでした。最近のニューラルレンダリングアルゴリズム【Tewari et al. 2022】は、こうしたアーティファクトを大幅に減らし、すべての入力画像をGPUに保存する圧倒的なコストを回避し、多くの点でこれらの方法を上回っています。

### 2.2 ニューラルレンダリングと放射場

ディープラーニング技術は、早い段階で新規ビュー合成に採用されました【Flynn et al. 2016; Zhou et al. 2016】。例えば、CNNはブレンドウェイトの推定に使用されたり【Hedman et al. 2018】、テクスチャ空間での解決策に利用されました【Riegler and Koltun 2020; Thies et al. 2019】。しかし、これらの多くの方法はMVSベースのジオメトリに依存する点が大きな欠点であり、さらに、最終的なレンダリングにCNNを使用することで、しばしば時間的なフリッカーが発生するという問題もあります。

新規ビュー合成におけるボリュメトリック表現は、Soft3D【Penner and Zhang 2017】によって始まり、続いてボリュメトリックレイマーチングと深層学習技術を組み合わせた手法が提案されました【Henzler et al. 2019; Sitzmann et al. 2019】。これらは、幾何学を表現するための連続的で微分可能な密度場を基盤としています。しかし、ボリュメトリックレイマーチングを使用したレンダリングは、ボリュームを照会するために多数のサンプルが必要であるため、コストがかさむという問題があります。Neural Radiance Fields (NeRFs)【Mildenhall et al. 2020】は、重要なサンプリングと位置エンコーディングを導入し品質を向上させましたが、大規模な多層パーセプトロン（MLP）を使用するため速度が低下しました。

NeRFの成功により、品質と速度に関する問題に取り組む多くの後続研究が爆発的に増加しました。その多くは正則化戦略を導入し、現時点で新規ビュー合成における画像品質の最先端はMip-NeRF360【Barron et al. 2022】です。この方法のレンダリング品質は卓越していますが、トレーニングとレンダリングには非常に長い時間がかかります。私たちの手法は、より速いトレーニングとリアルタイムレンダリングを提供しつつ、品質面でも同等またはそれ以上の成果を上げています。

最近の手法は主にトレーニングやレンダリングの速度向上に焦点を当て、3つの設計選択を活用しています。空間データ構造を使用して（ニューラル）特徴を保存し、それをボリュメトリックレイマーチング中に補間すること、異なるエンコーディングの使用、MLP容量の調整です。これらの方法には、空間の離散化を使用する様々なバリエーション【Chen et al. 2022b,a; Fridovich-Keil and Yu et al. 2022; Garbin et al. 2021; Hedman et al. 2021; Reiser et al. 2021; Takikawa et al. 2021; Wu et al. 2022; Yu et al. 2021】、コードブック【Takikawa et al. 2022】、およびハッシュテーブルのようなエンコーディング【Müller et al. 2022】が含まれており、これにより、より小さなMLPを使用するか、ニューラルネットワークを完全に省略することができます【Fridovich-Keil and Yu et al. 2022; Sun et al. 2022】。

最も注目すべき手法としては、InstantNGP【Müller et al. 2022】があり、これは計算を加速するためにハッシュグリッドと占有グリッドを使用し、密度と外観を表現するためにより小さなMLPを使用しています。また、Plenoxels【Fridovich-Keil and Yu et al. 2022】は、スパースボクセルグリッドを使用して連続的な密度場を補間し、ニューラルネットワークを完全に省略することが可能です。両方の方法は、方向効果を直接表現するために球面調和関数を使用していますが、両方ともシーンやキャプチャタイプによっては空間を効果的に表現するのに苦労することがあります。さらに、選択した構造化グリッドによって、画像品質が制約され、レンダリング速度も制限されます。私たちが使用する非構造化かつGPUに最適化された3Dガウスは、ニューラルコンポーネントを必要とせずに、より速いレンダリング速度とより高い品質を実現しています。

### 2.3 ポイントベースのレンダリングと放射場

ポイントベースの手法は、切り離された非構造化ジオメトリサンプル（ポイントクラウド）を効率的にレンダリングします【Gross and Pfister 2011】。最もシンプルな形態では、ポイントサンプルレンダリング【Grossman and Dally 1998】は、固定サイズの非構造化ポイントのセットをラスタライズします。これには、グラフィックスAPIでネイティブにサポートされているポイントタイプ【Sainz and Pajarola 2004】や、GPU上での並列ソフトウェアラスタライズ【Laine and Karras 2011; Schütz et al. 2022】を活用することができます。しかし、ポイントサンプルレンダリングは元のデータには忠実であるものの、穴が開いたり、エイリアシングが発生したり、連続性が厳密に保たれないという問題があります。高品質のポイントベースのレンダリングに関する先駆的な研究は、ピクセルよりも大きな範囲を持つ円形や楕円形のディスク、楕円体、またはサーフェル【Botsch et al. 2005; Pfister et al. 2000; Ren et al. 2002; Zwicker et al. 2001b】を使用して、これらの問題に対処しています。

近年、微分可能なポイントベースのレンダリング技術への関心が高まっています【Wiles et al. 2020; Yifan et al. 2019】。ポイントはニューラルな特徴で補強され、CNNを使用してレンダリングされることにより、速いまたはリアルタイムのビュー合成が可能になっています【Aliev et al. 2020; Rückert et al. 2022】。しかし、これらの手法は依然として初期のジオメトリにMVSに依存しているため、特に特徴が少ない/光沢のある領域や細い構造など、難しいケースでの過剰再構成や不十分な再構成といったMVS固有のアーティファクトを引き継いでしまいます。

ポイントベースの𝛼ブレンディングとNeRFスタイルのボリュメトリックレンダリングは、本質的に同じ画像生成モデルを共有しています。具体的には、色𝐶は以下のように放射レンダリングにより得られます:

\[
𝐶 = \sum_{i=1}^{N} 𝑇𝑖 (1 − \exp(−𝜎𝑖 𝛿𝑖)) c𝑖
\]
ここで、𝑇𝑖 = \exp\left(-\sum_{j=1}^{i-1} 𝜎𝑗 𝛿𝑗\right)
\]

この式は次のように書き換えることができます:

\[
𝐶 = \sum_{i=1}^{N} 𝑇𝑖 \alpha𝑖 c𝑖
\]
ただし、𝛼𝑖 = (1 − \exp(−𝜎𝑖𝛿𝑖)) であり、𝑇𝑖 = \prod_{j=1}^{i-1}(1 − 𝛼𝑗)です。

典型的なニューラルポイントベースのアプローチ（例:【Kopanas et al. 2022, 2021】）は、ピクセルの色𝐶を、ピクセルに重なるN個の順序付けられたポイントをブレンドすることで計算します:

\[
𝐶 = \sum_{i\in N} c𝑖𝛼𝑖 \prod_{j=1}^{i-1} (1 − 𝛼𝑗)
\]

ここで、c𝑖は各ポイントの色であり、𝛼𝑖は共分散Σを持つ2Dガウス分布の評価結果【Yifan et al. 2019】に学習されたポイントごとの不透明度を掛けたものです。

式 (2) と式 (3) から、画像生成モデルが同一であることが明らかです。しかし、レンダリングアルゴリズムは非常に異なっています。NeRFは、連続的な表現であり、空間の空/満を暗黙的に表現しています。そのため、式 (2) のサンプルを見つけるために高価なランダムサンプリングが必要となり、その結果、ノイズと計算コストが増大します。これに対して、ポイントは非構造化で離散的な表現であり、NeRFのようにジオメトリの生成、消去、配置の移動が可能な柔軟性を持っています。これを実現するためには、不透明度や位置の最適化が必要であり、これは以前の研究【Kopanas et al. 2021】によって示されていますが、完全なボリュメトリック表現の欠点は避けられます。

Pulsar【Lassner and Zollhofer 2021】は、高速な球体ラスタリゼーションを実現し、私たちのタイルベースおよびソーティングされたレンダラーにインスピレーションを与えました。しかし、前述の分析に基づき、我々は（近似的な）従来の𝛼ブレンディングをソートされたスプラットに維持し、ボリュメトリック表現の利点を享受したいと考えています。我々のラスタリゼーションは、順序に依存しないPulsarの手法とは異なり、視界順序を尊重します。さらに、全てのスプラットの勾配をバックプロパゲートし、異方性スプラットをラスタライズします。これらの要素は、我々の結果の高い視覚的品質に寄与しています（セクション7.3参照）。また、前述の方法の多くはレンダリングにCNNを使用しており、これが時間的な不安定性を引き起こします。それにもかかわらず、Pulsar【Lassner and Zollhofer 2021】やADOP【Rückert et al. 2022】のレンダリング速度は、我々の高速レンダリングソリューションの開発動機となりました。

Neural Point Catacaustics【Kopanas et al. 2022】のポイントベースのレンダリングトラックは、鏡面効果に焦点を当てていますが、MLPを使用してこの時間的不安定性を克服しています。しかし、依然としてMVSジオメトリを入力として必要としていました。最近の手法【Zhang et al. 2022】は、MVSを必要とせず、方向のために球面調和関数（SH）を使用していますが、一つのオブジェクトシーンのみを処理でき、初期化にはマスクが必要です。この手法は、小さな解像度と低ポイント数では高速ですが、通常のデータセット【Barron et al. 2022; Hedman et al. 2018; Knapitsch et al. 2017】に対応するためにどのようにスケールできるかは不明です。私たちは、より柔軟なシーン表現のために3Dガウシアンを使用しており、MVSジオメトリを必要とせず、プロジェクションされたガウシアンのタイルベースのレンダリングアルゴリズムのおかげでリアルタイムレンダリングを実現しています。

最近のアプローチ【Xu et al. 2022】は、放射輝度場をラジアル基底関数アプローチでポイントを用いて表現しています。この手法では、最適化中にポイントの剪定や密度化技術を使用していますが、ボリュメトリックレイマーチングを採用しており、リアルタイム表示速度は達成できません。

人間のパフォーマンスキャプチャの分野では、3Dガウシアンが人間の体を表現するために使用されています【Rhodin et al. 2015; Stoll et al. 2011】。最近では、視覚タスクにおいてボリュメトリックレイマーチングとともに使用されています【Wang et al. 2023】。また、類似の文脈では、ニューラルボリュメトリックプリミティブも提案されています【Lombardi et al. 2021】。これらの手法は、私たちがシーン表現として3Dガウシアンを選択する際にインスピレーションを与えましたが、それらは単一の孤立したオブジェクト（人体や顔）の再構築とレンダリングに焦点を当てており、奥行きの複雑さが小さいシーンを対象としています。

これに対して、我々の手法では、異方性共分散の最適化、最適化と密度制御を交互に行う手法、そしてレンダリングにおける効率的な深度ソートを組み合わせることで、屋内外を問わず大きな奥行きの複雑さを含む複雑なシーン全体（背景も含む）を処理することが可能です。



overview
私たちの手法への入力は、静的シーンの画像セットと、SfM（Structure-from-Motion）によってキャリブレーションされた対応するカメラのセットです【Schönberger and Frahm 2016】。SfMの副産物として得られるスパースポイントクラウドから、位置（平均）、共分散行列、及び opacity 𝛼 で定義される3Dガウシアンのセットを作成します（Sec. 4）。これにより非常に柔軟な最適化を行うことができ、3Dシーンの比較的コンパクトな表現を得ることができます。特に、高度に異方的なボリュメトリックスプラットを使用することで、細かい構造をコンパクトに表現することが可能です。

放射輝度場の方向性外観成分（色）は、標準的な手法に従い球面調和（SH）を用いて表現されます【Fridovich-Keil and Yu et al. 2022; Müller et al. 2022】。私たちのアルゴリズムは、3Dガウシアンのパラメータ（位置、共分散、𝛼、SH係数）を最適化する一連のステップを経て放射輝度場表現を作成します（Sec. 5）。この際、ガウシアンの密度を適応的に制御する操作を交互に行います。

私たちの手法の効率性の鍵は、タイルベースのラスタライザー（Sec. 6）にあります。このラスタライザーは、異方的スプラットの𝛼ブレンディングを行い、迅速なソートによって可視性の順序を尊重します。また、迅速なバックワードパスも含まれており、累積した𝛼値を追跡することで、勾配を受け取るガウシアンの数に制限はありません。私たちの手法の概要は、Fig. 2に示されています。



4 differentiavle 3d gaussian splatting
私たちの目標は、高品質の新規視点合成を可能にするシーン表現を最適化することであり、スパースなSfMポイントから始めることです。これを実現するためには、微分可能なボリュメトリック表現の特性を引き継ぎつつ、非常に高速なレンダリングを可能にするために非構造的かつ明示的なプリミティブが必要です。そこで、私たちは3Dガウシアンを選択しました。これは微分可能であり、2Dスプラットに簡単に投影できるため、レンダリングにおける高速な𝛼ブレンディングが可能です。

私たちの表現は、2Dポイントを使用する以前の方法【Kopanas et al. 2021; Yifan et al. 2019】と類似しており、各ポイントが法線を持つ小さな平面円であると仮定しています。しかし、SfMポイントの極端なスパース性により、法線を推定することは非常に困難です。同様に、そのような推定から非常にノイジーな法線を最適化することも困難です。そこで、私たちは、法線を必要としない3Dガウシアンのセットとして幾何学をモデル化します。私たちのガウシアンは、ワールドスペースで定義された完全な3D共分散行列Σで定義され、点（平均）𝜇を中心に持っています。

\[
G(x) = e^{-\frac{1}{2}(x)^T Σ^{-1}(x)}
\]

このガウシアンは、ブレンディングプロセスで𝛼と掛け合わされます。ただし、レンダリングのために3Dガウシアンを2Dに投影する必要があります。Zwickerら【2001a】は、この投影方法を画像空間に示しています。視点変換𝑊を考慮すると、カメラ座標系における共分散行列Σ'は次のように与えられます：

\[
Σ' = J W Σ W^T J^T
\]

ここで、𝐽は射影変換のアフィン近似のヤコビアンです。また、Zwickerらは、Σ'の第三行と第三列をスキップすることで、法線を持つ平面ポイントから始めた場合と同じ構造と特性を持つ2×2の分散行列を得られることを示しています【Kopanas et al. 2021】。

共分散行列Σを直接最適化して放射輝度場を表現する3Dガウシアンを得る明らかなアプローチがありますが、共分散行列は正定値半であるときに物理的意味を持ちます。私たちのパラメータ全体の最適化には勾配降下法を使用していますが、これはこのような有効な行列を生成するように簡単には制約できず、更新ステップや勾配は非常に簡単に無効な共分散行列を生成してしまいます。

そのため、私たちは最適化のためにより直感的でありながら同等に表現力のある表現を選択しました。3Dガウシアンの共分散行列Σは、楕円体の構成を記述することに類似しています。スケーリング行列𝑆と回転行列𝑅を考慮すると、対応するΣを次のように求めることができます：

\[
Σ = R S S^T R^T
\]

両方の要素を独立して最適化できるように、これらを別々に保存します：スケーリングのための3Dベクトル𝑠と、回転を表すクォータニオン𝑞です。これらはそれぞれの行列に簡単に変換でき、結合することで有効な単位クォータニオンを得るために𝑞を正規化します。

トレーニング中の自動微分による重大なオーバーヘッドを避けるために、すべてのパラメータの勾配を明示的に導出します。具体的な導関数計算の詳細は付録Aに記載されています。このような最適化に適した異方性共分散の表現により、3Dガウシアンを最適化してキャプチャされたシーンの異なる形状に適応させることができ、比較的コンパクトな表現を実現しています。Fig. 3はそのようなケースを示しています。



5 optimization with adaptive density control of 3d gaussians
私たちのアプローチの核心は、シーンを正確に表現する密な3Dガウシアンのセットを生成する最適化ステップです。このプロセスでは、位置 \( p \)、不透明度 \( \alpha \)、および共分散 \( \Sigma \) に加えて、各ガウシアンの色 \( c \) を表す球面調和（SH）係数も最適化し、シーンの視点依存の外観を正確にキャプチャします。これらのパラメータの最適化は、シーンをより良く表現するためにガウシアンの密度を制御するステップと交互に行われます。

### 5.1 最適化

最適化は、レンダリングの反復と、その結果得られた画像とキャプチャされたデータセットのトレーニングビューとの比較に基づいています。3Dから2Dへの投影の曖昧さのために、ジオメトリが誤って配置されることは避けられません。したがって、最適化は、誤って配置されたジオメトリを作成するだけでなく、破壊または移動する能力も必要です。3Dガウシアンの共分散のパラメータの品質は、表現のコンパクトさにとって重要であり、広い均質領域は、少数の大きな異方性ガウシアンでキャプチャできます。

最適化には確率的勾配降下法（SGD）を使用し、標準のGPU加速フレームワークを最大限に活用し、一部の操作に対してカスタムCUDAカーネルを追加することができるようにします。これは、最近のベストプラクティスに従ったものです【Fridovich-Keil and Yu et al. 2022; Sun et al. 2022】。特に、私たちの高速ラスタリゼーション（セクション6参照）は、最適化の効率において重要であり、最適化の主な計算ボトルネックとなります。

不透明度 \( \alpha \) にはシグモイド活性化関数を使用して、値を [0, 1) の範囲に制約し、滑らかな勾配を得るとともに、共分散のスケールには指数関数的活性化関数を使用して同様の理由を持ちます。初期共分散行列は、最も近い3つの点への距離の平均に等しい軸を持つ等方性ガウシアンとして推定します。

私たちは、Plenoxels【Fridovich-Keil and Yu et al. 2022】に似た標準的な指数減衰スケジューリング技術を使用しますが、位置にのみ適用します。損失関数は、L1とD-SSIM項の組み合わせで定義されます：

\[
L = (1 - \lambda)L_1 + \lambda L_{D-SSIM} \quad (7)
\]

すべてのテストで \( \lambda = 0.2 \) を使用します。学習スケジュールやその他の要素の詳細は、セクション7.1で提供します。

### 5.2 ガウシアンの適応制御

最初に、SfMから得られた初期のスパースポイントセットを使用し、次に、単位体積あたりのガウシアンの数と密度を適応的に制御する手法を適用します。これにより、初期のスパースガウシアンセットから、シーンをより適切に表現し、正しいパラメータを持つより密なセットへと進化させることができます。最適化のウォームアップ後（セクション7.1参照）、私たちは100イテレーションごとに密度を増加させ、透明度が閾値 \( \epsilon_\alpha \) よりも小さいガウシアンを削除します。

ガウシアンの適応的制御は、空白領域を埋める必要があります。これは、幾何学的特徴が不足している（「アンダーリコンストラクション」）領域や、シーンの大きな面積をカバーするガウシアン（しばしば「オーバーリコンストラクション」に相当）にも焦点を当てます。両者には大きなビュー空間の位置勾配が存在することが観察されます。直感的には、これはこれらの領域がまだ十分に再構築されていないことに起因し、最適化がこれを修正するためにガウシアンを移動させようとしているためです。

両方のケースが密度増加の良い候補であるため、私たちは位置勾配の平均の大きさが閾値 \( \tau_{\text{pos}} \) を超えるガウシアンを密度増加させます。実験的に、この閾値を0.0002に設定しました。このプロセスの詳細は、図4に示されています。

アンダーリコンストラクション領域にある小さなガウシアンについては、新しいジオメトリをカバーする必要があります。これには、同じサイズのガウシアンのコピーを作成し、その位置を勾配の方向に移動させることで複製するのが好ましいです。一方、高い分散がある領域にある大きなガウシアンは、より小さなガウシアンに分割する必要があります。このようなガウシアンは、2つの新しいガウシアンに置き換え、実験的に決定した \( \phi = 1.6 \) の因子でスケールを分割します。また、元の3DガウシアンをPDFとしてサンプリングして位置を初期化します。

最初のケースでは、システムの総体積とガウシアンの数の両方を増加させる必要があることを検出し、処理します。一方、2番目のケースでは、総体積は保存しながらガウシアンの数を増やします。その他のボリュメトリック表現と同様に、私たちの最適化は、入力カメラに近いフローティングオブジェクトで詰まる可能性があります。この場合、ガウシアン密度の不当な増加が生じる可能性があります。ガウシアンの数の増加を調整する効果的な方法は、3000イテレーションごとに \( \alpha \) 値をゼロ近くに設定することです。最適化は、必要なガウシアンの \( \alpha \) を増加させる一方で、上記で説明したように \( \epsilon_\alpha \) よりも小さい \( \alpha \) を持つガウシアンを削除することを許可します。ガウシアンは縮小または成長し、他のガウシアンと大きく重なることがありますが、非常に大きなワールドスペースのガウシアンやビュー空間における大きなフットプリントを持つガウシアンは定期的に削除します。この戦略により、ガウシアンの総数を全体的に良好に制御できます。私たちのモデル内のガウシアンは常にユークリッド空間の原始的なものであり、他の手法【Barron et al. 2022; Fridovich-Keil and Yu et al. 2022】とは異なり、遠方または大きなガウシアンのための空間圧縮、変形、または投影戦略を必要としません。

6 fast diffrenriable rasterizer for gaussians
### 6.1 タイルベースのラスタライザー

私たちの目標は、全体的なレンダリングの速度とソートの速度を向上させ、アニソトロピック・スプラットに対する近似的な \( \alpha \)-ブレンディングを実現し、以前の研究【Lassner and Zollhofer 2021】で存在した勾配を受け取るスプラットの数に対する厳しい制限を回避することです。この目標を達成するために、最近のソフトウェアラスタライゼーション手法【Lassner and Zollhofer 2021】からインスパイアを受けたガウシアン・スプラットのためのタイルベースのラスタライザーを設計しました。このラスタライザーは、全画像に対してプリソートを行うことで、以前の \( \alpha \)-ブレンディングソリューション【Kopanas et al. 2022, 2021】が妨げられたピクセルごとのソートのコストを回避します。

私たちの高速ラスタライザーは、追加のメモリ消費が少なく、低いオーバーヘッドで任意の数のブレンドされたガウシアンに対して効率的な逆伝播を可能にします。私たちのラスタライゼーションパイプラインは完全に微分可能であり、2Dへの投影（セクション4参照）を考慮すると、以前の2Dスプラッティング手法【Kopanas et al. 2021】と同様にアニソトロピックスプラットをラスタライズできます。

この方法では、画面を16×16のタイルに分割し、ビュー・フラスタムおよび各タイルに対して3Dガウシアンをカリングします。具体的には、99%の信頼区間がビュー・フラスタムと交差しているガウシアンのみを保持します。さらに、極端な位置にあるガウシアン（すなわち、近接平面に近く、ビュー・フラスタムの外にある平均値を持つもの）をトリビアルに拒否するためにガードバンドを使用します。これにより、彼らの投影された2D共分散を計算する際の不安定さを回避します。

次に、オーバーラップするタイルの数に応じて各ガウシアンをインスタンス化し、ビュー空間の深さとタイルIDを組み合わせたキーを各インスタンスに割り当てます。次に、GPUの高速ラディックスソート【Merrill and Grimshaw 2010】を使用して、これらのキーに基づいてガウシアンをソートします。注意すべき点は、追加のピクセルごとのポイントの順序付けは行われず、ブレンディングはこの初期ソートに基づいて実行されることです。その結果、私たちの \( \alpha \)-ブレンディングは、いくつかの設定において近似的なものとなる可能性があります。しかし、スプラットが個々のピクセルのサイズに近づくにつれて、これらの近似は無視できるものになります。この選択は、収束したシーンに目に見えるアーティファクトを生じさせることなく、トレーニングとレンダリングのパフォーマンスを大幅に向上させることがわかりました。

### 6.2 ラスタライゼーションの詳細

ガウシアンのソート後、各タイルに対して、特定のタイルにスプラットする最初および最後の深さソートされたエントリを特定することでリストを生成します。ラスタライゼーションのために、各タイルごとに1つのスレッドブロックを起動します。各ブロックは最初に協調的にガウシアンのパケットを共有メモリにロードし、その後、特定のピクセルに対してリストを前から後ろへと走査し、色と \( \alpha \) 値を累積します。これにより、データの読み込み/共有と処理の両方において並列性の向上が最大化されます。ピクセルでの \( \alpha \) の目標飽和に達した時点で、対応するスレッドは停止します。定期的に、タイル内のスレッドにクエリを実行し、全てのピクセルが飽和（つまり \( \alpha \) が1になる）すると、タイル全体の処理が終了します。ソートの詳細と全体のラスタライゼーションアプローチの高レベルの概要は、付録Cに記載されています。

ラスタライゼーション中、\( \alpha \) の飽和が唯一の停止基準となります。以前の研究とは異なり、勾配の更新を受けるブレンドされたプリミティブの数に制限を設けません。このプロパティを強制することで、任意の深度複雑性を持つシーンに対処し、正確に学習できるようになります。また、シーン固有のハイパーパラメータ調整に頼る必要がありません。

バックワードパス中は、フォワードパスでのブレンドされたポイントの完全なシーケンスをピクセルごとに復元する必要があります。一つの解決策は、グローバルメモリにピクセルごとのブレンドポイントの任意に長いリストを保存することです【Kopanas et al. 2021】。暗示される動的メモリ管理のオーバーヘッドを避けるために、私たちは再びパーセルリストを走査することを選択します。フォワードパスからのソートされたガウシアンの配列とタイル範囲を再利用できます。勾配計算を容易にするために、今回はバックトゥフロントで走査します。

走査は、タイル内の任意のピクセルに影響を与えた最後のポイントから始まります。また、ポイントの共有メモリへのロードは再び協調的に行われます。さらに、各ピクセルは、その深度がフォワードパス中に色に寄与した最後のポイントの深度以下である場合にのみ、（コストのかかる）オーバーラップテストおよびポイントの処理を開始します。セクション4で説明した勾配計算は、元のブレンディングプロセス中の各ステップでの累積オパシティ値を必要とします。バックワードパスで進行中のオパシティのリストを走査するのではなく、フォワードパスの最後に総累積オパシティのみを保存することで、これらの中間オパシティを復元できます。具体的には、各ポイントはフォワードプロセス中の最終累積オパシティ \( \alpha \) を保存します。バックトゥフロントの走査中に、これを各ポイントの \( \alpha \) で割ることで、勾配計算に必要な係数を取得します。

7