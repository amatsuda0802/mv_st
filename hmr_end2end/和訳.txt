abst
私たちは、HMR（Human Mesh Recovery）を紹介します。これは、単一のRGB画像から人間の体の完全な3Dメッシュを再構築するためのエンドツーエンドのフレームワークです。現在のほとんどの方法が2Dまたは3Dの関節位置を計算するのに対して、私たちは形状と3D関節角度でパラメータ化された、より豊かで有用なメッシュ表現を生成します。主な目的は、キーポイントの再投影誤差を最小化することであり、これにより、真実の2D注釈のみを持つ実世界の画像を使用してモデルをトレーニングできます。しかし、再投影誤差だけでは制約が非常に少ないため、この問題に対処するために、3D人間メッシュの大規模データベースを使用して、人体の形状とポーズパラメータが本物かどうかを判別する敵対的なトレーニングを導入します。

私たちのHMRは、ペアの2D-to-3Dスーパービジョンを使用せずにトレーニング可能であることを示しています。中間的な2Dキーポイントの検出に依存せず、画像ピクセルから直接3Dポーズと形状パラメータを推測します。モデルは、人物を含むバウンディングボックスが与えられれば、リアルタイムで動作します。私たちは、実世界のさまざまな画像でアプローチを示し、3Dメッシュを出力する以前の最適化ベースの手法を上回り、3D関節位置推定や部位セグメンテーションなどのタスクで競争力のある結果を示しています。





1 intro
私たちは、単一のRGB画像から人間の体の完全な3Dメッシュを再構築するためのエンドツーエンドのフレームワークを提案します。私たちは、メッシュを3D関節角度と低次元の線形形状空間でパラメータ化する生成的な人間体モデルであるSMPLを使用しています（[24]参照）。図1に示すように、3Dメッシュを推定することは、単純なスケルトンでは実用的ではない前景や部分セグメンテーションなどの広範なアプリケーションへの扉を開きます。出力メッシュは、アニメーターによって即座に使用され、変更され、測定され、操作され、リターゲティングされます。また、私たちの出力は全体的であり、遮蔽や切断のケースでも常に完全な3Dボディを推測します。

人間の体を単一の画像から3Dメッシュに再構築するための手法として、私たちは多くの研究が行われていることを指摘します。しかし、ほとんどのアプローチは3D関節位置の復元に焦点を当てています。私たちは、関節だけでは全体像が把握できないと主張します。関節は疎であり、人間の体は3D空間の表面によって定義されます。また、関節位置だけでは各関節の自由度（DoF）全体を制約することができません。これにより、3D関節位置だけから体全体のポーズを推定することは容易ではありません。対照的に、私たちは運動連鎖の各関節に対する相対的な3D回転行列を出力し、3D頭部や四肢の向きに関する情報を捉えます。回転を予測することで、四肢の対称性と有効な長さが確保されます。私たちのモデルは、3Dボディモデルのデータセットから関節の角度制限を暗黙的に学習します。

現在の3D人間メッシュ復元の手法は、多段階アプローチに依存しています[5, 20]。まず、2D関節位置を推定し、次にそれらから3Dモデルパラメータを推定します。このステップバイステップのアプローチは最適ではなく、ここでは画像ピクセルからモデルパラメータに直接マッピングを学習するエンドツーエンドの解決策を提案します。しかし、このようなモデルをエンドツーエンドでトレーニングするにはいくつかの課題があります。まず、現実世界の画像に対する大規模な真実3Dアノテーションが不足しています。正確な3Dアノテーションを持つ既存のデータセットは制約された環境でキャプチャされており、これらのデータセットで訓練されたモデルは、実際の画像の豊かさにうまく一般化しません。もう一つの課題は、単一視点の2Dから3Dへのマッピングに固有のあいまいさです。最もよく知られているのは深度のあいまいさであり、複数の3Dボディ構成が同じ2D投影を説明する可能性があります[42]。これらの構成の多くは、関節の不可能な角度や極端に痩せた体など、人体にとって合理的ではありません。また、カメラを明示的に推定することは、人物のサイズとカメラ距離の間に追加のスケールあいまいさを導入します。

本論文では、これらの課題に対処する新しいメッシュ再構築アプローチを提案します。重要な洞察は、現実世界の画像の大規模な2Dキーポイントアノテーションと、さまざまなポーズと形状を持つ3Dメッシュの別の大規模データセットが存在することです。私たちの重要な貢献は、これらの未ペアの2Dキーポイントアノテーションと3Dスキャンを条件付き生成敵対的な方法で活用することです。具体的には、画像を与えられたとき、ネットワークは3Dメッシュパラメータとカメラを推定し、これにより3Dキーポイントがアノテーションされた2Dキーポイントと一致するようにします。あいまいさに対処するために、これらのパラメータは識別ネットワークに送られます。識別ネットワークの役割は、3Dパラメータが実際の人間のボディに対応しているかどうかを判断することです。したがって、ネットワークは人間のマニフォールド上でパラメータを出力するように奨励され、識別器は弱い監視として機能します。ネットワークは各関節の角度制限を暗黙的に学習し、異常な体型の人物を作ることを抑制されます。

体モデルパラメータを予測する際の追加の課題は、回転行列への回帰が難しいことです。ほとんどのアプローチは、角度をビンに分割することによって回転推定を分類問題として定式化しています[45]。しかし、再投影誤差に関して角度確率を微分することは簡単ではなく、離散化は精度を犠牲にします。そこで、私たちはこれらの値をフィードバックとともに反復的に回帰することを提案します。私たちのフレームワークは図2に示されています。

私たちのアプローチは、再投影損失を使用した3Dインタープリターネットワーク[33, 50]や、敵対的逆グラフィックスネットワーク[47]の最近の技術と似ていますが、既存の技術を複数の方法で超えています：

1. 画像の特徴から直接3Dメッシュパラメータを推測しますが、従来のアプローチは2Dキーポイントから推測します。これにより、二段階のトレーニングの必要がなくなり、文脈などの画像の貴重な情報を失うことがありません。
2. スケルトンを超え、メッシュを出力します。これはより複雑で多くのアプリケーションに適しています。また、追加の推論ステップは必要ありません。
3. フレームワークはエンドツーエンドの方法で訓練されています。
4. 2Dから3Dへのペアデータを使用せずに、ペアデータを使用した場合と使用しない場合の結果を示します。ペアデータを使用しなくても、私たちのアプローチは合理的な3D再構築を生成します。これは、大量の2Dデータから3Dを学習する可能性を開く非常にエキサイティングなものです。

現実世界の画像からの人間の3Dメッシュ再構築を評価するためのデータセットが存在しないため、私たちは標準的な3D関節位置推定タスクでアプローチを評価します。私たちのアプローチは、2D関節からSMPLパラメータを推定する以前の手法よりも優れており、3Dスケルトンのみを出力するアプローチとも競争力があります。また、人間の部位セグメンテーションの補助タスクに対しても評価を行いました。挑戦的な現実世界の画像に対する私たちのアプローチの定性的な評価を行い、異なる誤差パーセンタイルでサンプリングした結果を示します。私たちのモデルとコードは、研究目的で利用可能です（https://akanazawa.github.io/hmr/）。





2 related work
3Dポーズ推定は、画像やビデオシーケンスから体の主要な3D関節を位置づける問題として定式化されることが多いですが、この「ポーズ」の概念は過度に単純化されていると私たちは考えています。ポーズ推定のアプローチは、大きく二つのカテゴリに分かれます：二段階アプローチと直接推定アプローチです。

### 二段階アプローチ
二段階アプローチでは、まず2Dポーズ検出器[30, 49, 54]を使用して2D関節位置を予測し、次に2D関節から3D関節位置を推定します。この推定は、回帰[26, 29]やモデルフィッティングによって行われます。一般的な手法は、3Dスケルトンの学習辞書を利用することです[2, 34, 47, 37, 53, 54]。これらの方法は、2Dから3Dへの推定における固有のあいまいさを制約するために、さまざまな先行情報を使用します[42]。多くの手法は、肢の長さや比率について何らかの仮定をします[4, 21, 32, 34]。AkhterとBlack[2]は、ポーズ依存の関節角度制限を捉えた新しいポーズの先行情報を学習します。二段階アプローチはドメインシフトに対してより堅牢であるという利点がありますが、2D関節検出に過度に依存し、3Dポーズの推定において画像情報を失う可能性があります。

### 直接推定アプローチ
HumanEva[38]やHuman3.6M[16]のような動きキャプチャデータセットでは、問題は3D関節位置の観点で定義されており、これにより3D関節推定問題は標準的な教師あり学習問題として定式化できます。したがって、最近の多くの手法は深層学習フレームワーク[33, 43, 44, 51, 52]を用いて、画像から直接3D関節を推定します。支配的なアプローチは完全畳み込みであり、Xiao et al.[40]のように骨を回帰して3Dポーズベンチマークで優れた結果を得る方法もあります。多くの方法はカメラを推定せず、ルートに対して深度を推定し、骨の平均長に基づく事前定義されたグローバルスケールを使用します[33, 51, 52]。最近、Rogez et al.[36]は、人間検出と3Dポーズ予測を組み合わせています。これらの直接推定方法の主な問題は、正確な真実の3Dアノテーションを持つ画像が制御されたMoCap環境でキャプチャされていることです。これらの画像のみに基づいて訓練されたモデルは、現実世界にうまく一般化しません。

### 弱監視による3D推定
最近の研究では、MoCapと現実世界の画像の間のドメインギャップの問題に対処するために、エンドツーエンドのフレームワークが提案されています。RogezとSchmid[35]は、MoCapデータを用いて2Dポーズアノテーションを持つ画像に人工的に3Dアノテーションを付与します。いくつかの手法[27, 28, 51]は、現実世界のデータセットとMoCapデータセットを共同で訓練します。さらに、他の手法[27, 28]は、事前に訓練された2Dポーズネットワークを使用し、2Dポーズ予測を補助タスクとして利用します。3Dアノテーションが利用できない場合、Zhou et al.[51]は、相対的な骨の長さを一定に保つように促す幾何学的制約から弱監視を得ます。本研究では、3D関節角度と3D形状を出力し、肢が対称であるべきというこれらの制約を包含します。私たちは、敵対的な先行情報を使用して、より強力な形式の弱監視を採用します。

### 3D関節以外を出力する手法
手動で抽出されたシルエットにパラメトリックボディモデルをフィットさせる複数の手法[8]や、手動で提供されたいくつかの対応点[12, 14]があります。最近の研究では、この作業を自動化しようとする試みが行われています。Bogo et al.[5]は、14の検出された2D関節からSMPLパラメータを回復する最適化ベースの手法SMPLifyを提案しています。この手法は複数の先行情報を活用しています。しかし、最適化ステップのため、このアプローチはリアルタイムではなく、画像ごとに20〜60秒を要します。また、彼らは関節角度制限について事前に仮定を設けています。Lassner et al.[20]は、SMPLifyからのキュレーションされた結果を利用して、伝統的な身体の関節や表面上の点に対応する91のキーポイント検出器を訓練します。その後、彼らはキーポイントにフィットさせるためにSMPLモデルパラメータを最適化します。彼らはまた、SMPLパラメータを直接回帰するためのランダムフォレスト回帰アプローチを提案し、精度を犠牲にしながらランタイムを短縮します。私たちのアプローチは、検出された2Dキーポイントからではなく、画像から直接SMPLパラメータを推測し、リアルタイムで動作するため、両手法を上回ります。

VNect [28]は、推定された2Dおよび3D関節位置に対してリグ付きスケルトンモデルを時間的にフィットさせます。彼らは最適化後に各関節の3D回転を回復できますが、私たちは画像から回転を直接出力し、表面頂点も出力します。同様に、Zhou et al. [52]は、固定された運動学的ツリーの関節回転を直接回帰します。私たちは形状やカメラスケールも出力し、3Dポーズ推定において彼らのアプローチを上回ります。

SMPL関連の出力を予測する他の関連手法もあります。Varol et al. [48]は、レンダリングされたSMPLボディの合成データセットを使用して、深度および身体部分のセグメンテーションのための完全畳み込みモデルを学習します。DenseReg [13]も同様に、人体のための密な対応マップを出力します。これらはすべて基盤となる3Dボディの2.5D投影です。本研究では、すべてのSMPLパラメータとカメラを回復し、これらすべての出力を得ることができます。

Kulkarni et al. [19]は、ボディ形状とポーズの生成モデルを使用して、単一の画像からボディポーズを推定するための確率的プログラミングフレームワークを使用します。彼らは視覚的に単純な画像に対処し、3Dポーズの精度を評価しません。最近では、Tan et al. [41]が、まず合成データを使用してSMPLパラメータのシルエットデコーダを学習し、その後デコーダを固定した状態で画像エンコーダを学習してシルエット再投影損失を最小化します。しかし、シルエットに依存するため、彼らのアプローチは正面画像や隠蔽のない人間の画像に制限されます。同時に、Tung et al. [46]は、画像と一連の2D関節ヒートマップからSMPLパラメータを予測します。このモデルは合成データセットで事前訓練され、テスト時に2つの連続したビデオフレームで再投影損失を最小化するためにファインチューニングされます。

私たちのアプローチは、ペア付きの監視なしで訓練でき、入力として2D関節ヒートマップを必要とせず、ファインチューニングなしで画像をテストします。さらに、私たちは混乱や隠蔽のある現実世界の画像[22]でもアプローチを示しています。





3 model
我々は、RGB画像Iから人間のフル3Dメッシュを直接再構築することを提案します。このプロセスはフィードフォワード方式で行われます。トレーニング中、すべての画像には地上真実の2D関節が注釈されていると仮定します。また、一部の画像には3D注釈がある場合も考慮します。さらに、さまざまな形状とポーズの人間の3Dメッシュのプールが存在することを仮定します。これらのメッシュは必ずしも対応する画像を持たないため、このデータは「未対」(unpaired)と呼ばれます【55】。

図2は、提案されたネットワークアーキテクチャの概要を示しています。このアーキテクチャはエンドツーエンドで訓練可能です。画像の畳み込み特徴は、3D回帰モジュールに送られます。このモジュールの目的は、3D関節が注釈された2D関節に投影されるように、3D人体とカメラを推定することです。推定されたパラメータは、3Dパラメータが未対データからの実際のメッシュであるかどうかを判断するための敵対的識別ネットワークにも送信されます。これにより、ネットワークは人間の体の多様なメッシュを出力するよう促され、地上真実の3D注釈がない野生画像に対する弱い監視として機能します。

3Dメッシュモデルの豊かな表現により、このデータ駆動型の先行知識は、関節角度の制限、人体計測の制約（例：身長、体重、骨の比率）をキャプチャし、3D関節位置のみを予測するモデルが使用する幾何学的な先行知識を包含します【34, 40, 51】。地上真実の3D情報が利用可能な場合は、それを中間損失として使用することもできます。全体の目的関数は次のように表されます。

\[
L = \lambda (L_{\text{reproj}} + 1L_{3D}) + L_{\text{adv}} \tag{1}
\]

ここで、λは各目的の相対的重要性を制御し、1は画像に対する地上真実の3Dが利用可能な場合に1、そうでない場合に0となる指示関数です。我々は3D損失がある場合とない場合の結果を示します。それぞれの構成要素については、次に詳しく説明します。



3.1 3d body representation
私たちは、Skinned Multi-Person Linear（SMPL）モデル[24]を使用して人間の3Dメッシュをエンコードします。SMPLは生成モデルであり、人間の体を形状（身長、体重、体の比率の変動）とポーズ（関節による3D表面の変形）に分解します。形状 \(\beta \in \mathbb{R}^{10}\) は、主成分分析（PCA）形状空間の最初の10係数によってパラメータ化されます。ポーズ \(\theta \in \mathbb{R}^{3K}\) は、K = 23の関節の相対的な3D回転を軸-角（axis-angle）表現でモデル化します。SMPLは微分可能な関数であり、N = 6980の頂点を持つ三角形メッシュを出力します。これは、\(\beta\) と \(\theta\) に条件付けられたテンプレートボディ頂点を形作り、その後、関節回転 \(\theta\) に従ってボーンを前方運動学により関節化し、最後に線形ブレンドスキニングで表面を変形させることによって得られます。再投影誤差に使用される3Dキーポイント \(X(\theta, \beta) \in \mathbb{R}^{3 \times P}\) は、最終メッシュ頂点から線形回帰を用いて取得されます。

我々は弱透視カメラモデルを採用し、軸-角表現での全体回転 \(R \in \mathbb{R}^{3 \times 3}\)、平行移動 \(t \in \mathbb{R}^2\)、スケール \(s \in \mathbb{R}\) を求めます。したがって、人間の3D再構築を表すパラメータのセットは、次のように85次元ベクトルとして表現されます：

\[
\Theta = \{ \theta, \beta, R, t, s \}
\]

与えられた \(\Theta\) に対して、\(X(\theta, \beta)\) の投影は次のように表されます：

\[
\hat{x} = s\Pi(RX(\theta, \beta)) + t, \tag{2}
\]

ここで、\(\Pi\) は正射影（orthographic projection）を意味します。



3.2 iterative 3d regression with feedback
3D回帰モジュールの目標は、画像エンコーディング \(\phi\) に基づいて \(\Theta\) を出力し、関節再投影誤差 \(L_{\text{reproj}}\) を最小化することです。この再投影誤差は次のように定義されます：

\[
L_{\text{reproj}} = \sum_{i} ||v_i(x_i - \hat{x}_i)||_1, \tag{3}
\]

ここで、\(x_i \in \mathbb{R}^{2 \times K}\) は \(i\) 番目のグラウンドトゥルース 2D 関節、\(v_i \in \{0, 1\}^K\) は各関節 \(K\) の可視性（可視の場合は1、そうでない場合は0）を示します。しかし、\(\Theta\) を一度で直接回帰することは特に難しい課題です。これは、\(\Theta\) に回転パラメータが含まれているためです。本研究では、先行研究[7, 9, 31]からインスピレーションを得て、反復誤差フィードバック（IEF）ループで \(\Theta\) を回帰します。このループでは、現在の推定値に進行的な変化を繰り返し加えます。具体的には、3D回帰モジュールは画像特徴 \(\phi\) と現在のパラメータ \(\Theta_t\) を入力として受け取り、残差 \(\Delta \Theta_t\) を出力します。このパラメータは、残差を現在の推定値に加えることによって更新されます：

\[
\Theta_{t+1} = \Theta_t + \Delta \Theta_t.
\]

初期推定値 \(\Theta_0\) は、平均値 \(\bar{\Theta}\) に設定されます。[7, 31] では、推定値が画像空間にレンダリングされ、画像入力と連結されますが、本研究ではすべてを潜在空間に保持し、単純に特徴 \([\phi, \Theta]\) を回帰器への入力として連結します。このアプローチがうまく機能し、パラメータの微分可能なレンダリングが難しい場合に適していることを発見しました。

ペアのグラウンドトゥルース 3D データが利用可能な場合、追加の直接 3D スーパービジョンを利用することができます。最も一般的な形式の3Dアノテーションは、3D関節です。生の3Dモーションキャプチャマーカーデータが利用可能な場合、MoSh[23, 48]を通じてSMPLパラメータ[\(\beta, \theta\)]を取得できます。以下に3D損失の定義を示します。私たちは、直接スーパービジョン \(L_3D\) を使用した場合と使用しなかった場合の結果を示します：

\[
L_3D = L_{\text{joints}} + L_{\text{smpl}}. \tag{4}
\]

\[
L_{\text{joints}} = ||(X_i - \hat{X}_i)||_2^2 \tag{5}
\]

\[
L_{\text{smpl}} = ||[\beta_i, \theta_i] - [\hat{\beta}_i, \hat{\theta}_i]||_2^2. \tag{6}
\]

[7, 31] は、各反復で回帰出力を監視するために「制約された」補正ターゲットを使用します。しかし、これはグラウンドトゥルース推定が常に知られていると仮定するものであり、私たちのセットアップでは多くの画像にグラウンドトゥルース 3D アノテーションがないため、成り立ちません。これらのアプローチが指摘するように、最終目的で各反復を監視することは、レグレッサーがオーバーシュートし、局所的最小値に引っかかる原因となります。したがって、私たちは最終推定 \(\Theta_T\) に対してのみ \(L_{\text{reproj}}\) と \(L_3D\) を適用し、すべての反復推定 \(\Theta_t\) に対しては敵対的損失を適用し、ネットワークが3D人体のマニフォールドに沿った修正ステップを取るように促します。



3.3 factorized adversarial prior
再投影損失は、ネットワークが2D関節位置を説明する3Dボディを生成することを奨励しますが、人類学的に不合理な3Dボディや大きな自己交差を持つボディも再投影損失を最小化する可能性があります。この問題を正則化するために、SMPLパラメータが実際のボディに対応しているかどうかを判断するために訓練された識別器ネットワーク \(D\) を使用します。これを、3D推論をガイドするデータ駆動型の事前知識として「敵対的先行」と呼びます[47]。

SMPLのような豊かで明示的な3D表現を使用するもう1つの利点は、潜在空間の意味を正確に把握できることです。特に、SMPLは因子化された形状とポーズの構造を持っているため、敵対的先行をデータ効率良く、安定して訓練できるように活用できます。具体的には、形状とポーズの因子分解を反映し、形状とポーズに対して独立した識別器を訓練します。ポーズは運動学的木に基づいているため、ポーズの識別器をさらに分解し、各関節の回転ごとに1つずつ訓練します。これにより、各関節の角度制限を学習することになります。全運動学的木の関節分布をキャプチャするために、すべての回転を入力とする識別器も学習します。

各識別器への入力は非常に低次元（\(\beta\) に対して10次元、各関節に対して9次元、すべての関節に対して9K次元）であるため、それぞれ小さなネットワークを持つことができ、訓練が非常に安定します。すべてのポーズ識別器は回転行列の共通特徴空間を共有し、最終的な分類器のみが別々に学習されます。

以前のアプローチが関節の限界に対して事前に仮定を行うのに対して[5, 52]、私たちは運動学的スケルトンモデルの自由度を事前に定義しません。代わりに、因子化された敵対的先行を通じてデータ駆動型の方法で学習されます。この因子化がなければ、ネットワークはポーズと形状を適切に正則化することを学ばず、視覚的に不快な結果を生み出します。敵対的先行の重要性は、ペアの3Dスーパービジョンが利用できない場合に非常に重要です。敵対的先行がなければ、ネットワークは完全に制約のない人間のボディを生成します（セクション4.3で示します）。

モード崩壊はGANにおいて一般的な問題ですが[10]、ネットワークは識別器を欺くだけでなく、再投影誤差も最小化しなければならないため、この問題に悩まされることはありません。画像にはすべてのモードが含まれており、ネットワークはそれらすべてに一致させられます。因子化は、未見のボディ形状とポーズの組み合わせに一般化できるため、モード崩壊を回避するのに役立つ可能性があります。

最終的に、私たちは \(K + 2\) の識別器を訓練します。各識別器 \(D_i\) は、\(\Theta\) がデータから来た確率を表す[0, 1]の間の値を出力します。実際には、安定性のために最小二乗の定式化[25]を使用します。エンコーダ \(E\) を画像エンコーダと3Dモジュールを含むものとすると、エンコーダに対する敵対的損失関数は次のようになります：

\[
\min L_{\text{adv}}(E) = \sum_{i} E_{\Theta \sim p_E} \left[ (D_i(E(I)) - 1)^2 \right], \tag{7}
\]

そして各識別器の目的は次のようになります：

\[
\min L(D_i) = E_{\Theta \sim p_{\text{data}}} \left[ (D_i(\Theta) - 1)^2 \right] + E_{\Theta \sim p_E} \left[ D_i(E(I))^2 \right]. \tag{8}
\]

私たちは \(E\) とすべての \(D\) を共同で最適化します。



3.4 implementation details
### データセット

使用する野外画像データセットは、2DキーポイントでアノテーションされたLSP、LSP-extended [17]、MPII [3]、およびMS COCO [22]です。画像が小さすぎるか、6つ未満の可視キーポイントを持つ画像をフィルタリングし、それぞれ1k、10k、20k、80kの画像のトレーニングセットを取得します。すべてのテスト結果は、真のバウンディングボックスを使用して得られます。

3Dデータセットには、Human3.6M [16] およびMPI-INF-3DHP [28]を使用します。MPI-INF-3DHPのトレーニング被験者8のシーケンスは、ハイパーパラメータを調整するための検証セットとして除外し、最終実験にはフルトレーニングセットを使用します。これらのデータセットは制御された環境でキャプチャされており、3D関節アノテーションを持つ150kのトレーニング画像を提供します。Human3.6Mでは、生の3Dモーションキャプチャマーカーを用いてトレーニング画像の真のSMPLパラメータをMoSh [23]を使用して取得します。敵対的先行を訓練するために使用される未ペアデータは、3つのモーションキャプチャデータセットからのMoShデータに基づいています：CMU [6]、Human3.6Mトレーニングセット [16]、およびPosePriorデータセット [2]で、極端なポーズの広範なバリエーションを含みます。これらはそれぞれ390k、150k、180kのサンプルからなります。

すべての画像はアスペクト比を維持しながら224 × 224にスケールされ、タイトなバウンディングボックスの対角線が約150pxになるようにします（[17]を参照）。画像はランダムにスケーリング、平行移動、および反転されます。ミニバッチサイズは64です。ペアの3Dスーパービジョンが使用される場合、各ミニバッチは2Dサンプルと3Dサンプルの半分で構成されるようにバランスを取ります。すべての実験は、特に指定がない限り、ペアの3D損失を使用してすべてのデータセットで行われます。

SMPLのK = 23関節の定義は、これらのデータセットで使用される一般的な関節定義と完全に一致しません。私たちは[5, 20]に従い、再構成されたメッシュからHuman3.6Mの14関節を取得するために回帰器を使用します。また、MS COCOデータセット[22]から5つの顔のキーポイントも組み込みます。新しいキーポイントは、対応する頂点IDを指定することでメッシュ表現に簡単に組み込むことができます。合計で再投影誤差はP = 19のキーポイントに対して計算されます。

### アーキテクチャ

画像エンコーディングにはResNet-50ネットワーク[15]を使用し、ImageNet分類タスク[39]で事前トレーニングを行います。ResNetの出力は平均プーリングされ、特徴量 \(\phi \in \mathbb{R}^{2048}\) を生成します。3D回帰モジュールは、1024ニューロンの完全連結層が2層、間にドロップアウト層を配置し、最後に85次元のニューロン層を持ちます。すべての実験でT = 3回の反復を使用します。

形状の識別器は、10、5、1のニューロンを持つ2つの完全連結層から構成されます。ポーズでは、\(\theta\)を最初にRodriguesの公式を使用してK個の3 × 3回転行列に変換します。各回転行列は、32の隠れニューロンを持つ2つの完全連結層の共通埋め込みネットワークに送信されます。次に、出力はK = 23の異なる識別器に送信され、それぞれ1次元の値を出力します。全体のポーズ分布の識別器は、すべてのK \(*\) 32の表現を別の2つの完全連結層で連結し、最終的に1次元の値を出力します。すべての層はReLU活性化を使用し、最終層のみが異なります。エンコーダと識別器ネットワークの学習率はそれぞれ\(1 \times 10^{-5}\)と\(1 \times 10^{-4}\)に設定されています。Adamソルバー[18]を使用し、55エポックにわたってトレーニングします。単一のTitan 1080ti GPUでのトレーニングには約5日かかります。\(\lambda\)やその他のハイパーパラメータは、MPI-INF-3DHPデータセットの検証データを通じて設定されます。実装はTensorFlow[1]を使用しています。





4 experimental results
### 結果の評価

我々は3Dスケルトン以上の情報を回収しますが、現在のデータセットには真の3Dアノテーションメッシュが存在しないため、結果の評価は困難です。したがって、標準的な3D関節推定タスクにおいて定量的な評価を行います。また、補助タスクとして体の部位セグメンテーションも評価します。

図1では、MS COCO [22]の困難な画像に対する定性的結果を示しています。これには、遮蔽、混雑、切断、複雑なポーズが含まれます。モデルが頭部や四肢の向きを回復する様子が見られます。図3では、Human3.6M、MPI-INF-3DHP、LSP、MS COCOのテストセットにおけるさまざまな誤差パーセンタイルでの結果を示しています。我々のアプローチは、95パーセンタイルの誤差においても合理的な再構成を回復しています。さらなる結果については、プロジェクトのウェブサイトをご覧ください 。すべての図において、ペアの2D-to-3Dスーパービジョンを使用したモデルの結果は淡い青色で、使用していないモデルの結果は淡いピンク色で示されています。



4.1 3d joint location estimation
### 3D関節誤差の評価

**Human3.6M**  
我々は、ラボ環境でキャプチャされた標準的な3DポーズベンチマークであるHuman3.6Mにおける3D関節誤差を評価します。また、より多様なポーズや俳優の外観をカバーするMPI-INF-3DHP [27]とも比較します。このデータセットはより多様ですが、野外画像の複雑さと豊かさにはまだ遠く及びません。

3D関節誤差の評価に使用されるいくつかの誤差指標を報告します。最も一般的な評価は、平均関節位置誤差（MPJPE）および再構成誤差であり、これは予測を真の値と剛体整列させた後のMPJPEです（プロクルステス解析による）。再構成誤差は、全体的な不整合を除去し、再構築された3Dスケルトンの品質を評価します。

#### Human3.6Mの評価

評価には2つの一般的なプロトコルを使用します。最初のプロトコルP1では、5人の被験者（S1、S5、S6、S7、S8）でトレーニングし、2人（S9、S11）でテストします。以前の研究[33, 36]に従い、すべてのビデオを50fpsから10fpsにダウンサンプリングして冗長性を減少させます。2番目のプロトコルP2は、同じトレーニング/テストセットを使用しますが、正面カメラ（カメラ3）のみでテストし、再構成誤差を報告します。

表1では、P2における我々の手法（HMR）の結果を、単一の画像からSMPLパラメータを出力する2つの最近のアプローチ[5, 20]と比較しています。どちらのアプローチも2Dキーポイント検出を入力として必要とし、我々は両方を大きく上回っています。表2では、P1における結果も示しています。ここでも、同様に3D関節角度を出力する最近のアプローチであるZhou et al. [52]を上回っています。彼らは各関節の自由度（DoF）を手動で指定しますが、我々はこれをデータから学習します。また、彼らは固定の骨長を仮定しますが、我々は形状を解決します。HMRは、3D関節位置のみを予測する最近の最先端の手法に対して競争力があります。

MPJPEは、結果の視覚的品質とあまり相関しないようです。高MPJPEの多くの結果は、図3に示すように非常に合理的に見えることがわかります。図3では、さまざまな誤差パーセンタイルでの結果を示しています。

**MPI-INF-3DHP**  
MPI-INF-3DHPのテストセットは、7つのアクションを実行する6人の被験者からの2929の有効フレームで構成されています。このデータセットは、マルチカメラのマーカーレスモーションキャプチャシステムを使用して屋内外で収集されています。そのため、真の3Dアノテーションにはいくつかのノイズが含まれています。MPJPEに加えて、150mmで閾値を設定した正しいキーポイントの割合（PCK）およびさまざまなPCK閾値に対する曲線の下の面積（AUC）も報告します[27]。

結果は表3に示されています。すべての手法は[27]による透視補正を使用しています。また、公開されているコード[28]を使用して、HMRおよびVNectの剛体整列後のメトリックも報告します。VNectの結果は、時間を通じての後処理最適化なしで報告します。再び、我々は3D関節を出力するようにトレーニングされたアプローチに対して競争力があり、剛体整列後にVNectを改善しています。



4.2 human body segmentation
### ヒトの体のセグメンテーションに関する評価

我々のアプローチを、LSP [17]の1000枚のテスト画像における補助タスクであるヒトの体のセグメンテーションでも評価します。これらの画像には、スポーツをする人々の複雑なポーズが含まれており、6つの体部位セグメントと背景に関するラベルが付けられています。LSPにはトレーニング用の真の3Dラベルが利用できない点に注意が必要です。トレーニング時にセグメンテーションラベルは使用していません。

我々は、セグメンテーション精度と、背景を含むすべての部位の平均F1スコアを報告します[20]と同様に、前景-背景のセグメンテーションに関する結果も報告します。SMPLメッシュの部位定義のセグメンテーションは、アノテーションのそれとは正確には一致しないため、最高の精度が100%未満に制限される点に注意してください。

結果は表4に示されています。我々の結果は、真のセグメンテーションとキーポイントを最適化ターゲットとして使用するSMPLifyオラクル [20]と比較可能です。また、[20]の決定木よりも優れた性能を示しています。HMRはバウンディングボックスが与えられた場合、リアルタイムで動作する点も特筆すべきです。



4.3 without paired 3d supervision
### 無対3D監視下でのHMRの評価

ここまで、ペアリングされた2D-to-3Dの監視、すなわちL3Dが利用可能な場合に使用してきました。ここでは、ペアリングされた3D監視なしで訓練されたモデルを評価します。この設定を「HMR無対」と呼び、すべてのテーブルに数値結果を報告します。3D関節推定タスクの結果を報告するすべての手法は、直接的な3D監視に依存しており、それなしでは訓練できません。再投影損失に基づく手法でさえも[33, 47, 50]、ペアリングされた2D-to-3Dの訓練データを必要とします。

この困難な設定にもかかわらず、結果は驚くほど競争力があります。対抗的先行知識がペアリングされた2D-to-3Dデータなしでの訓練にとって不可欠である点に注意してください。図5は、ペアリングされた3D監視も対抗的損失も使用せずに訓練されたモデルが、極端な形状やポーズを持つ「モンスター」を生成することを示しています。2Dデータの量を増やすことで、3D精度が大幅に向上するかどうかは、今後の研究課題として残されています。





5 conclusion
この論文では、単一のRGB画像から人体の完全な3Dメッシュモデルを復元するためのエンドツーエンドのフレームワークを提案します。メッシュは、3D関節角度と低次元の線形形状空間に基づいてパラメータ化されており、さまざまな実用的なアプリケーションがあります。過去数年間、制御された環境でキャプチャされた画像における単一ビュー3Dポーズ予測に急速な進展が見られました。これらのベンチマークでのパフォーマンスは飽和し始めていますが、野外画像からの3D人体再構築に関してはあまり進展がありません。ペアリングされた3Dデータを使用せずに得られた結果は有望であり、これは、自然な環境で得るのが非常に困難なグラウンドトゥルース3Dデータの代わりに、比較的簡単に取得できる2Dラベル付きの画像を用いてモデルを改善し続けることができることを示唆しています。

### 謝辞
N. MahmoodにモーションキャプチャデータへのSMPLモデルのフィッティングとキャラクターアニメーションのためのメッシュリターゲティングを感謝します。D. MehtaにはMPI-INF-3DHPに関する支援を、S. Tulsiani、A. Kar、S. Gupta、D. Fouhey、Z. Liuには有益な議論をしていただき感謝します。この研究はBAIRのスポンサーとNSFの助成金IIS-1526234の支援を受けました。