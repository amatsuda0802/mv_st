abst
野外でのマルチビュー・ステレオ再構築（MVS）は、まずカメラの内部・外部パラメータを推定する必要があります。これらのパラメータの取得は通常、面倒で煩雑な作業ですが、3D空間で対応するピクセルを三角測量するためには不可欠であり、最高性能のMVSアルゴリズムの核心です。本研究では、これに対して異なるアプローチを取り、カメラのキャリブレーションや視点の事前情報がなくても動作可能な、任意の画像コレクションに対する高密度で自由なステレオ3D再構築を実現するDUSt3Rを導入します。従来の射影カメラモデルの厳密な制約を緩和し、ペアワイズな再構築問題をポイントマップの回帰として定式化しました。この定式化は、単眼および双眼の再構築ケースを滑らかに統一します。2枚以上の画像が提供される場合には、各ペアのポイントマップを共通の参照フレームに表現するシンプルで効果的なグローバルアライメント戦略を提案しています。

ネットワークアーキテクチャは、標準的なトランスフォーマーのエンコーダとデコーダに基づいており、強力な事前学習モデルを活用しています。この定式化により、シーンの3Dモデルや深度情報が直接提供されるだけでなく、ピクセルマッチング、相対的および絶対的なカメラ位置もシームレスに復元できます。これらのタスクに対する徹底的な実験により、提案するDUSt3Rがさまざまな3Dビジョンタスクを統一し、単眼およびマルチビューの深度推定や相対ポーズ推定で新たな最先端の性能を実現できることを示しました。まとめると、DUSt3Rは多くの幾何学的3Dビジョンタスクを簡単にします。





1 intro
複数の視点からの制約のない画像ベースの高密度な3D再構築は、コンピュータビジョンの長年にわたる研究目標の一つです。このタスクは、特定のシーンの一連の写真を与えられたときに、そのシーンの3Dジオメトリとカメラパラメータを推定することを目指しています。この技術は、地図作成、ナビゲーション、考古学、文化遺産の保存、ロボティクスなどの多くの応用を持つだけでなく、全ての3Dビジョンタスクの中でも特に重要な位置を占めています。実際、このタスクは他のほとんどの幾何学的な3Dビジョンタスクを内包しているためです。

現代の3D再構築アプローチは、キーポイント検出、マッチング、ロバストな推定、構造復元（SfM）やバンドル調整（BA）、高密度マルチビューステレオ（MVS）など、さまざまなサブ分野の数十年にわたる進展を組み合わせることから成り立っています。最終的に、現代のSfMおよびMVSパイプラインは、一連の最小問題を解決することに帰着します。具体的には、ポイントのマッチング、エッセンシャルマトリクスの見つけ方、ポイントの三角測量、シーンのスパース再構築、カメラの推定、そして高密度な再構築の順に処理が進みます。

この複雑なプロセスは、特定の環境下では有効な解決策ですが、各サブ問題が完璧に解決されていないため、次のステップにノイズを加えることになり、全体のパイプラインを機能させるための複雑さや工学的努力が増していることが課題です。特に、各サブ問題間の連携が欠けていることが問題です。例えば、カメラのポーズを回復するために構築されたスパースシーンから、高密度な再構築が自然に恩恵を受けるべきであり、逆もまた然りです。

さらに、このパイプラインの重要なステップは多くの場合壊れやすく、失敗しやすいことが知られています。例えば、すべてのカメラパラメータを推定するための重要な段階であるSfMは、シーンの視点数が少ない場合や、非ランバート的な表面を持つオブジェクトの場合、またはカメラの動きが不十分な場合など、多くの一般的な状況で失敗することがよくあります。

本論文では、未校正および未設定のカメラから高密度な無制約ステレオ3D再構築を行うための、全く新しいアプローチ「DUSt3R」を提案します。このアプローチの主な要素は、シーンやカメラに関する事前情報（カメラの内部パラメータさえも）なしに、画像ペアからのみ高密度で正確なシーン表現を回帰できるネットワークです。このシーン表現は、3Dポイントマップに基づいており、次のような豊富な特性を同時に内包しています：(a) シーンのジオメトリ、(b) ピクセルとシーンポイントの関係、(c) 2つの視点間の関係。この出力からだけで、カメラやシーンのジオメトリといったシーンパラメータのほぼすべてが簡単に抽出可能です。これは、ネットワークが入力画像と生成された3Dポイントマップを同時に処理し、2D構造と3D形状を関連付けることを学習し、複数の最小問題を同時に解決する機会を持つため、内部で「協力」することができるからです。

モデルは完全に教師あり学習で訓練され、シンプルな回帰損失を使用し、合成的に生成された正解アノテーションや、SfMソフトウェアで再構築されたもの、専用のセンサーを使用して取得されたデータを持つ大規模な公開データセットを活用しています。我々は、タスク固有のモジュールを統合する傾向から離れ、汎用的なトランスフォーマーアーキテクチャに基づく完全なデータ駆動戦略を採用し、推論時には幾何学的な制約を課すことなく、強力な事前学習スキームを活用します。ネットワークは、MVS（マルチビューステレオ）でよく使われる形状の事前知識（テクスチャ、陰影、輪郭からの形状推定など）に似た、強力な幾何学的・形状の事前知識を学習します。

複数の画像ペアからの予測を統合するために、バンドル調整（BA）をポイントマップのケースに再適用し、これにより完全なMVS（Multi-View Stereo）を実現します。我々は、従来のBAとは異なり、再投影誤差を最小化するのではなく、カメラの姿勢とジオメトリの整合性を3D空間内で直接最適化するグローバルアライメント手法を導入しました。これは、高速であり、実際のシナリオで優れた収束性を示しています。実験では、様々な未知のセンサーを用いた実生活シナリオにおいて、再構築が正確で視点間で一貫性があることが示されています。また、同じアーキテクチャが単眼およびマルチビューの再構築シナリオにシームレスに対応できることも確認しました。再構築の例は、図1および添付のビデオで示されています。

要約すると、我々の貢献は以下の4点です。第一に、単眼および双眼の3D再構築を統合する、未校正および未設定の画像からのエンドツーエンドの3D再構築パイプラインを初めて提示しました。第二に、MVSアプリケーション向けのポイントマップ表現を導入し、ネットワークがピクセルとシーンの間の暗黙の関係を維持しつつ、標準フレーム内で3D形状を予測できるようにしました。これにより、通常の透視カメラの制約の多くを解消します。第三に、マルチビュー3D再構築の文脈でポイントマップをグローバルに整列させるための最適化手法を導入しました。この手法により、従来のSfM（Structure-from-Motion）およびMVSパイプラインのすべての中間出力を簡単に抽出することが可能です。ある意味では、このアプローチはすべての3Dビジョンタスクを統合し、従来の再構築パイプラインよりもはるかに簡素化し、DUSt3Rをシンプルで容易に見えるものにします。第四に、単眼およびマルチビューの深度ベンチマーク、ならびにマルチビューのカメラ姿勢推定において、最先端の性能を実現したことを示しています。





2 related work
スペースの都合上、ここでは3Dビジョンにおける最も関連する研究を要約し、より包括的なレビューについては付録のセクションCを参照してください。

**Structure-from-Motion (SfM)** [20, 21, 44, 47, 105] は、画像のセットからカメラパラメータを同時に決定しつつ、スパースな3Dマップを再構築することを目指しています。従来のパイプラインは、複数の画像間でキーポイントを一致させることで得られたピクセル対応に基づいて幾何学的関係を決定し、それに続いてバンドル調整を行い、3D座標とカメラパラメータを同時に最適化します。近年では、このSfMパイプラインに学習ベースの技術が取り入れられ、大幅な改良が行われています。これには、洗練された特徴記述 [26, 28, 96, 134, 166]、より正確な画像マッチング [3, 17, 59, 81, 99, 119, 125, 144]、フィーチャーメトリックの改良 [58]、ニューラルバンドル調整 [57, 152] などが含まれます。しかし、SfMパイプラインの逐次的な構造は変わらず、個々のコンポーネントにおけるノイズやエラーに弱いという課題があります。

**Multi-View Stereo (MVS)** は、複数の視点間での三角測量を通じて、可視表面を高密度に再構築するタスクです。従来のMVSの定式化では、すべてのカメラパラメータが入力として提供されることが前提です。完全に手作りの手法 [32, 34, 106, 146, 174]、シーン最適化に基づく最近の手法 [31, 70, 75, 76, 142, 145, 147, 162]、および学習ベースの手法 [52, 64, 85, 160, 163, 179] のいずれも、データ取得時の複雑なキャリブレーション手続き [1, 23, 108, 165]、または自然環境下での再構築のためにStructure-from-Motionアプローチ [47, 105] を使用して得られたカメラパラメータ推定に依存しています。しかし、実際のシナリオでは、事前に推定されたカメラパラメータの不正確さが、これらのアルゴリズムの正しい動作を妨げる要因となることがあります。本研究では、カメラパラメータに関する明示的な知識を持たずに、可視表面のジオメトリを直接予測することを提案します。

**RGBから3Dへの直接変換**  
最近、単一のRGB画像から直接3Dジオメトリを予測するアプローチが提案されています。この問題は追加の仮定を導入しなければ本質的に不定形であるため、これらの手法では、大規模なデータセットから強力な3D事前情報を学習するニューラルネットワークを活用して曖昧さを解消します。これらの手法は2つのグループに分類できます。

1つ目のグループは、クラスレベルのオブジェクト事前情報を活用するものです。たとえば、Pavlloら [82–84] は、大量の2D画像コレクションが与えられた場合、単一の画像から形状、姿勢、外観を完全に再現できるモデルの学習を提案しています。このタイプのアプローチは強力ですが、未見のカテゴリのオブジェクトに対して形状を推論することはできません。

2つ目のグループは、一般的なシーンに焦点を当てており、私たちの手法に最も近いものです。これらの手法は、既存のモノキュラ深度推定 (MDE) ネットワーク [6, 90, 168, 170] を体系的に構築または再利用します。深度マップは確かに3D情報の一形態をエンコードしており、カメラの内部パラメータと組み合わせることで、ピクセルに整列した3Dポイントクラウドを生成できます。たとえば、SynSin [150] は、カメラパラメータが既知である場合に、特徴拡張された深度マップをレンダリングすることで、単一の画像から新しい視点を合成します。

カメラの内部パラメータが不明な場合、1つの解決策は、映像フレーム間の時間的一貫性を利用してそれらを推定することで、グローバルアライメントを強制する [155]、またはフォトメトリック再構成ロスを用いた微分可能なレンダリングを活用する [36, 116] といった方法があります。もう一つの方法は、カメラの内部パラメータを明示的に予測するように学習し、MDEと組み合わせることで単一の画像からメトリック3D再構築を行うことです [167, 169]。

しかし、これらの方法はすべて、深度推定の品質に本質的に制限されており、モノキュラ設定では問題が不定形であるため、これが課題となります。

**対照的に、私たちのネットワークは2つの視点を同時に処理して、深度マップではなくポイントマップを出力します。** 理論上、これにより異なる視点からの光線間での三角測量が可能になります。3D再構築のためのマルチビュー・ネットワークは過去にも提案されており、これらは本質的に微分可能なSfMパイプラインを構築し、従来のパイプラインをエンドツーエンドでトレーニングするというアイデアに基づいています [130, 135, 183]。しかし、そのためには正確なカメラの内部パラメータが入力として必要であり、出力は通常、深度マップと相対カメラポーズです [135, 183]。一方、私たちのネットワークは一般的なアーキテクチャを持ち、カメラポーズを暗黙的に扱うポイントマップ、つまり3Dポイントの密な2Dフィールドを出力します。これにより、回帰問題がはるかに良好に定義されます。

**ポイントマップ**  
ポイントマップの集合を形状表現として使用することは、MVSにとっては非常に直感に反するものですが、視覚的ローカライゼーションタスクでは広く使用されています。これには、シーン依存の最適化アプローチ [8, 9, 11] や、シーンに依存しない推論手法 [95, 123, 158] などが含まれます。同様に、視点ごとのモデリングは、モノキュラ3D再構築の研究 [56, 112, 126, 140] や、視点合成の研究 [150] でも共通のテーマです。このアイデアは、複数の視点で標準的な3D形状を保存し、画像空間で作業することを目的としています。これらのアプローチでは、通常、標準的な表現のレンダリングを通じて、明示的な透視投影カメラのジオメトリを活用します。





3 method
私たちの手法の詳細に入る前に、まずポイントマップの基本的な概念について説明します。

**ポイントマップ（Pointmap）**  
ここでは、3Dポイントの密な2Dフィールドをポイントマップ \( X \in \mathbb{R}^{W \times H \times 3} \) と定義します。解像度 \( W \times H \) の対応するRGB画像 \( I \) と組み合わせることで、画像のピクセルと3Dシーンのポイント間に一対一の対応が形成されます。すなわち、すべてのピクセル座標 \( (i, j) \in \{1, \ldots, W\} \times \{1, \ldots, H\} \) に対して、\( I_{i,j} \leftrightarrow X_{i,j} \) となります。ここでは、各カメラの光線が単一の3Dポイントにヒットするものと仮定しており、半透明の表面のケースは無視しています。

**カメラとシーン**  
カメラの内部パラメータ \( K \in \mathbb{R}^{3 \times 3} \) が与えられた場合、観測されたシーンのポイントマップ \( X \) は、次のように真の深度マップ \( D \in \mathbb{R}^{W \times H} \) から簡単に得られます。

\[
X_{i,j} = K^{-1} [iD_{i,j}, jD_{i,j}, D_{i,j}]^\top
\]

ここで、\( X \) はカメラ座標系で表現されています。以下では、カメラ \( n \) のポイントマップ \( X_n \) をカメラ \( m \) の座標系で表現したものを \( X_{n,m} \) とします。

\[
X_{n,m} = P_m P_n^{-1} h(X_n)
\]

ここで、\( P_m, P_n \in \mathbb{R}^{3 \times 4} \) は画像 \( n \) と \( m \) のワールド座標系からカメラ座標系への変換行列であり、\( h : (x, y, z) \to (x, y, z, 1) \) は同次変換を表します。



3.1 overview
私たちは、直接回帰を通じて一般化されたステレオケースにおける3D再構築タスクを解決するネットワークを構築したいと考えています。そのために、2つのRGB画像 \( I_1, I_2 \in \mathbb{R}^{W \times H \times 3} \) を入力として受け取り、対応する2つのポイントマップ \( X_{1,1}, X_{2,1} \in \mathbb{R}^{W \times H \times 3} \) と関連する信頼度マップ \( C_{1,1}, C_{2,1} \in \mathbb{R}^{W \times H} \) を出力するネットワーク \( F \) を訓練します。両方のポイントマップは \( I_1 \) の同じ座標系で表現されており、これは既存のアプローチとは根本的に異なりますが、重要な利点を提供します（詳細はセクション1、2、3.3および3.4を参照）。明確さのために、かつ一般化を失わないように、両方の画像が同じ解像度 \( W \times H \) を持つと仮定しますが、実際には解像度が異なることもあります。

**ネットワークアーキテクチャ**  
私たちのネットワーク \( F \) のアーキテクチャは、CroCo [149] にインスパイアされたもので、CroCoの事前学習 [148] から大いに恩恵を受けることができるように設計されています。図2に示すように、ネットワークは各画像に対して同一の分岐を持ち、それぞれに画像エンコーダー、デコーダー、回帰ヘッドを含んでいます。最初に、2つの入力画像は、同じ重みを共有するViTエンコーダー [27] によってSiamese方式でエンコードされ、次の2つのトークン表現を得ます。

\[
F_1 = \text{Encoder}(I_1), \quad F_2 = \text{Encoder}(I_2)
\]

ネットワークは、その後、デコーダ内で両方のトークンを共同で処理します。CroCo [149] と同様に、デコーダはクロスアテンションを備えた汎用のトランスフォーマーネットワークです。各デコーダブロックは、最初に自己アテンション（各視点のトークンが同じ視点のトークンに注意を向ける）を実行し、次にクロスアテンション（各視点のトークンが他の視点のすべてのトークンに注意を向ける）を行い、最後にトークンをMLPに送ります。重要なのは、デコーダの通過中に常に両方の分岐間で情報が共有されることです。これは、適切に整列したポイントマップを出力するために重要です。具体的には、各デコーダブロックは他の分岐のトークンに注意を向けます。

\[
G^1_i = \text{DecoderBlock}^1_i(G^1_{i-1}, G^2_{i-1}),
\]
\[
G^2_i = \text{DecoderBlock}^2_i(G^2_{i-1}, G^1_{i-1}),
\]

ここで \( i = 1, \ldots, B \) は \( B \) ブロックのあるデコーダを示し、初期化はエンコーダトークン \( G^1_0 := F^1 \) と \( G^2_0 := F^2 \) から行われます。ここで、\( \text{DecoderBlock}^v_i(G^1, G^2) \) は分岐 \( v \in \{1, 2\} \) の \( i \) 番目のブロックを示し、\( G^1 \) と \( G^2 \) は入力トークンであり、\( G^2 \) は他の分岐のトークンです。最後に、各分岐では別々の回帰ヘッドがデコーダトークンのセットを受け取り、ポイントマップと関連する信頼度マップを出力します。

\[
X_{1,1}, C_{1,1} = \text{Head}^1(G^1_0, \ldots, G^1_B),
\]
\[
X_{2,1}, C_{2,1} = \text{Head}^2(G^2_0, \ldots, G^2_B)
\]

**議論**  
出力されるポイントマップ \(X_{1,1}\) と \(X_{2,1}\) は、未知のスケールファクターに対して回帰されます。また、我々の汎用アーキテクチャは、明示的に幾何学的制約を強制することはありません。そのため、ポイントマップは必ずしも物理的に妥当なカメラモデルに対応するわけではありません。むしろ、ネットワークには、幾何学的に一貫性のあるポイントマップのみを含むトレーニングセットから、存在するすべての関連する事前情報を学習させることを許可しています。汎用アーキテクチャを使用することで、強力な事前学習技術を活用でき、最終的には既存のタスク特化型アーキテクチャが達成できるものを上回ることが可能です。学習プロセスの詳細については、次のセクションで説明します。



3.2 training objective
**3D回帰損失**  
我々の唯一のトレーニング目標は、3D空間における回帰に基づいています。地上真実のポイントマップを \( \bar{X}_{1,1} \) および \( \bar{X}_{2,1} \) とし、これらは式 (1) に基づいて得られます。また、地上真実が定義されている有効なピクセルの2つの対応するセットを \( D_1 \) と \( D_2 \) で表します。ビュー \( v \in \{1, 2\} \) の有効なピクセル \( i \in D_v \) に対する回帰損失は、単純にユークリッド距離として定義されます：

\[
\ell_{\text{regr}}(v, i) = \left\| \frac{1}{z} X_{v,1}^{i} - \frac{1}{\bar{z}} \bar{X}_{v,1}^{i} \right\| \tag{2}
\]

予測値と地上真実の間のスケールの曖昧さを処理するために、予測されたポイントマップと地上真実のポイントマップは、それぞれスケールファクター \( z = \text{norm}(X_{1,1}, X_{2,1}) \) と \( \bar{z} = \text{norm}(\bar{X}_{1,1}, \bar{X}_{2,1}) \) で正規化します。これらは単に、すべての有効な点の原点からの平均距離を表します：

\[
\text{norm}(X_{1}, X_{2}) = \frac{1}{|D_1| + |D_2|} \sum_{v \in \{1,2\}} \sum_{i \in D_v} \| X_v^{i} \| \tag{3}
\]

**信頼度に基づく損失**  
実際には、我々の仮定とは異なり、空や半透明の物体のように、定義が不十分な3Dポイントが存在します。一般的に、画像内の一部の領域は他の領域よりも予測が難しい傾向があります。したがって、各ピクセルに対してネットワークがこの特定のピクセルについてどれだけの信頼を持っているかを表すスコアを予測することを共同で学習します。最終的なトレーニング目標は、すべての有効ピクセルに対する信頼度重み付き回帰損失です：

\[
L_{\text{conf}} = \sum_{v \in \{1,2\}} \sum_{i \in D_v} C_{v,1}^i \ell_{\text{regr}}(v, i) - \alpha \log C_{v,1}^i, \tag{4}
\]

ここで、\( C_{v,1}^i \) はピクセル \( i \) に対する信頼度スコアであり、\( \alpha \) は正則化項を制御するハイパーパラメータです【136】。信頼度が厳密に正となるように、通常、次のように定義します：

\[
C_{v,1}^i = 1 + \exp(C_{g}^{v,1}) > 1.
\]

これにより、ネットワークが1つの視点で覆われたような難しい領域で外挿することを強制されます。この目標でネットワーク \( F \) をトレーニングすることで、明示的な監視なしで信頼度スコアを推定できるようになります。入力画像ペアとそれに対応する出力の例は、図3および付録の図4、5、8に示されています。



3.3 downstream applications
出力されたポイントマップの豊富な特性により、さまざまな便利な操作を簡単に実行できるようになります。

### ポイントマッチング
2つの画像間でピクセルの対応関係を確立することは、3Dポイントマップ空間内での最近傍（NN）探索によって簡単に行えます。誤差を最小限に抑えるために、通常、画像 \( I_1 \) と \( I_2 \) の間で相互（双方向）対応 \( M_{1,2} \) を保持します。すなわち、次のようになります：

\[
M_{1,2} = \{(i, j) | i = \text{NN}_{1,2}^1(j) \text{ and } j = \text{NN}_{2,1}^1(i)\}
\]

ここで、\( \text{NN}_{n,m}^k(i) = \arg \min_{j \in \{0,...,WH\}} \| X_{n,k}^j - X_{m,k}^i \| \) です。

### 内部パラメータの復元
定義上、ポイントマップ \( X_{1,1} \) は画像 \( I_1 \) の座標系で表現されています。したがって、単純な最適化問題を解くことでカメラの内部パラメータを推定することが可能です。本研究では、主点がほぼ中心にあることとピクセルが正方形であると仮定しているため、推定すべき焦点 \( f^*_1 \) のみが残ります：

\[
f^*_1 = \arg \min_{f_1} \sum_{i=0}^{W} \sum_{j=0}^{H} C_{1,1}^{i,j} \left( \frac{(i', j') - f_1 (X_{1,1}^{i,j,0}, X_{1,1}^{i,j,1})}{X_{1,1}^{i,j,2}} \right),
\]

ここで \( i' = i - \frac{W}{2} \) および \( j' = j - \frac{H}{2} \) です。Weiszfeldアルゴリズムに基づく高速な反復ソルバーを使用すれば、最適な \( f^*_1 \) を数回の反復で見つけることができます。2つ目のカメラの焦点 \( f^*_2 \) に関しては、単純にペア \( (I_2, I_1) \) に対して推論を行い、上記の式を \( X_{2,2} \) に適用することで取得できます。

### 相対ポーズの推定
相対ポーズの推定は、いくつかの方法で達成できます。一つの方法は、2Dマッチングを行い、上記のように内部パラメータを復元した後、エピポーラ行列を推定して相対ポーズを回復することです【44】。もう一つの、より直接的な方法は、ポイントマップ \( X_{1,1} \) と \( X_{1,2} \)（または同等に \( X_{2,2} \) と \( X_{1,2} \)）を比較し、Procrustesアライメント【63】を使用して相対ポーズ \( P^* = [R^* | t^*] \) を得ることです：

\[
R^*, t^* = \arg \min_{\sigma,R,t} \sum_{i} C_{1,1}^i C_{1,2}^i \| \sigma(R X_{1,1}^i + t) - X_{1,2}^i \|^2.
\]

これは閉形式で達成可能ですが、Procrustesアライメントはノイズや外れ値に敏感です。より堅牢な解決策として、RANSAC【30】とPnP【44, 51】に依存することが考えられます。

### 絶対ポーズの推定
絶対ポーズの推定、すなわち視覚的ローカリゼーションも、いくつかの異なる方法で実現できます。クエリ画像を \( I_Q \)、参照画像を \( I_B \) とすると、2D-3Dの対応関係が利用可能です。最初に、\( I_Q \) の内部パラメータを \( X_{Q,Q} \) から推定します。1つの可能性は、\( I_Q \) と \( I_B \) の間で2D対応関係を取得し、それにより \( I_Q \) の2D-3D対応関係を得てからPnP-RANSAC【30, 51】を実行することです。別の解決策は、前述のように \( I_Q \) と \( I_B \) の間の相対ポーズを取得することです。その後、このポーズを世界座標に変換し、\( X_{B,B} \) と \( I_B \) のグラウンドトゥースポイントマップとのスケールに応じて適切にスケーリングします。



3.4 global alignment
これまでに紹介したネットワーク \( F \) は、ペアの画像しか処理できませんでした。ここでは、複数の画像から予測されたポイントマップを共同の3D空間にアラインするための、迅速かつシンプルな後処理最適化を提案します。これは、設計上、2つの整列したポイントクラウドとそれに対応するピクセルから3Dマッピングを含む、出力ポイントマップの豊富な内容のおかげで可能になります。

### ペアワイズグラフ
あるシーンに対する画像の集合 \(\{I_1, I_2, \ldots, I_N\}\) を考え、まず接続性グラフ \(G(V, E)\) を構築します。ここで、\(N\) 枚の画像が頂点 \(V\) を形成し、各エッジ \(e = (n, m) \in E\) は画像 \(I_n\) と \(I_m\) が何らかの視覚的内容を共有していることを示します。この目的のために、既存のオフ・ザ・シェルフの画像検索方法を使用するか、すべてのペアをネットワーク \(F\) に通し（推論は H100 GPU で約 40ms かかります）、両方のペアの平均信頼度に基づいてオーバーラップを測定し、低信頼度のペアをフィルタリングします。

### グローバル最適化
接続性グラフ \(G\) を使用して、すべてのカメラ \(n = 1 \ldots N\) のグローバルに整列したポイントマップ \(\{\chi_n \in \mathbb{R}^{W \times H \times 3}\}\) を回復します。この目的のために、各画像ペア \(e = (n, m) \in E\) に対して、ペアワイズポイントマップ \(X_{n,n}\)、\(X_{m,n}\) とそれに対応する信頼度マップ \(C_{n,n}\)、\(C_{m,n}\) を予測します。明確にするために、以下のように定義します：

\[
X_{n,e} := X_{n,n}, \quad X_{m,e} := X_{m,n}
\]

我々の目標は、すべてのペアワイズ予測を共通の座標系で回転させることです。そのため、各ペア \(e \in E\) に関連付けられたペアワイズポーズ \(P_e \in \mathbb{R}^{3 \times 4}\) とスケーリング \(\sigma_e > 0\) を導入します。そして、次の最適化問題を定式化します：

\[
\chi^* = \arg \min_{\chi, P, \sigma} \sum_{e \in E} \sum_{v \in e} HW \sum_{i=1}^{HW} C_{v,e}^i \|\chi_v^i - \sigma_e P_e X_{v,e}^i\| 
\]

ここで、記法を悪用し、\(v \in e\) の場合は \(v \in \{n, m\}\) とします。この式のアイデアは、与えられたペア \(e\) に対して、同じ剛体変換 \(P_e\) が、両方のポイントマップ \(X_{n,e}\) と \(X_{m,e}\) をワールド座標ポイントマップ \(\chi_n\) と \(\chi_m\) に整列させるべきだということです。これらの \(X_{n,e}\) と \(X_{m,e}\) は定義上、同じ座標系で表現されています。全ての \(e \in E\) に対して \(\sigma_e = 0\) という自明な最適解を回避するため、\(\prod_e \sigma_e = 1\) を強制します。

### カメラパラメータの復元
このフレームワークに対する単純な拡張により、すべてのカメラパラメータを復元することができます。単に次のように置き換えることで、

\[
\chi_{n}^{i,j} := P^{-1}_n h(K_n^{-1}[i D_n^{i,j}; j D_n^{i,j}; D_n^{i,j}])
\]

(すなわち、式 (1) のように標準的なカメラのピンホールモデルを強制します)、すべてのカメラポーズ \(\{P_n\}\)、関連する内部パラメータ \(\{K_n\}\)、および深度マップ \(\{D_n\}\) を \(n = 1 \ldots N\) に対して推定することができます。

### 討論
従来のバンドル調整とは異なり、このグローバル最適化は実際には迅速かつシンプルに実行できます。実際、我々は通常、バンドル調整が行うように2D再投影誤差を最小化するのではなく、3D投影誤差を最小化しています。この最適化は標準的な勾配降下法を使用して行われ、通常は数百ステップ後に収束し、標準的なGPU上で数秒を要します。





4 experiments with dust3r
### トレーニングデータ
私たちは、Habitat、MegaDepth、ARKitScenes、Static Scenes 3D、Blended MVS、ScanNet++、CO3D-v2、Waymo の8つのデータセットの混合を使用してネットワークをトレーニングします。これらのデータセットは、屋内、屋外、合成、実世界、オブジェクト中心など、多様なシーンタイプを特徴としています。データセットに直接提供されていない画像ペアは、[148] で説明されている方法に基づいて抽出します。具体的には、既存の画像検索およびポイントマッチングアルゴリズムを利用して、画像ペアをマッチングし、検証します。合計で8.5Mペアを抽出しました。

### トレーニング詳細
各エポック中に、データセットサイズの不均衡を均等化するために、各データセットから同数のペアをランダムにサンプリングします。ネットワークに比較的高解像度の画像（最大辺512ピクセル）を供給したいと考えていますが、そのような入力に関連するコストを軽減するため、224×224の画像で最初にトレーニングし、その後に512ピクセルの大きな画像でトレーニングを行います。各バッチごとに画像のアスペクト比をランダムに選択（例：16/9、4/3など）し、テスト時にネットワークが異なる画像形状に慣れているようにします。単に画像を希望するアスペクト比にクロップし、最大辺が512ピクセルになるようにリサイズします。

標準的なデータ拡張技術と全体的なトレーニングセットアップを使用しています。ネットワークアーキテクチャは、エンコーダーにVit-Large、デコーダーにViT-Base、DPTヘッドを採用しています。トレーニングとアーキテクチャの詳細については、付録のセクションFを参照してください。トレーニング前に、オフ・ザ・シェルフのCroCo事前トレーニングモデル[148]の重みでネットワークを初期化します。Cross-View completion（CroCo）は、MAE[45]に触発された最近提案された事前トレーニングの枠組みであり、さまざまな下流の3Dビジョンタスクで優れていることが示されており、私たちのフレームワークに特に適しています。セクション4.6では、CroCo事前トレーニングの影響と画像解像度の向上についてアブレーションを行います。

### 評価
このセクションの残りの部分では、DUSt3Rを代表的な古典的3Dビジョンタスクのセットでベンチマークし、各回にデータセット、メトリックを指定し、既存の最先端のアプローチと性能を比較します。すべての結果は同じDUSt3Rモデルで得られていることを強調します（デフォルトモデルは「DUSt3R 512」と呼ばれ、他のDUSt3Rモデルはセクション4.6のアブレーションに使用されます）。つまり、特定の下流タスクでモデルを微調整することはありません。テスト中、すべてのテスト画像はアスペクト比を保持しつつ512ピクセルにリスケールされます。DUSt3Rからタスク固有の出力を抽出するための異なる「ルート」が存在する可能性があるため、セクション3.3および3.4で説明されているように、使用された方法を毎回明確にします。

### 定性的結果
DUSt3Rは、困難な状況でも高品質な密な3D再構築を提供します。ペアワイズおよび多視点の再構築の非選択的な視覚化については、付録のセクションBを参照してください。



4.1 visual localization
### データセットとメトリック
まず、7Scenes [113] および Cambridge Landmarks データセット [48] における絶対ポーズ推定タスクに対して DUSt3R を評価します。7Scenes には、動画からの RGB-D 画像とそれに対応する 6-DOF カメラポーズを持つ 7 つの屋内シーンが含まれています。Cambridge Landmarks には、RGB 画像とそれに関連するカメラポーズを持つ 6 つの屋外シーンが含まれており、これらは SfM によって取得されています。結果として、中央値の平行移動と回転の誤差を (cm/°) で報告します。

### プロトコルと結果
世界座標系におけるカメラポーズを計算するために、DUSt3R をクエリ画像と最も関連性の高いデータベース画像との間の 2D-2D ピクセルマッチャー (セクション 3.3 を参照) として使用します。つまり、クエリ画像 I_Q とデータベース画像 I_B から出力された生のポイントマップを、何の修正も行わずに単純に利用します。Cambridge Landmarks にはトップ 20 の取得画像、7Scenes にはトップ 1 の取得画像を使用し、既知のクエリ内部パラメータを活用します。真の内部パラメータを使用しない結果については、付録のセクション E を参照してください。

各シーンについて、既存の最先端技術と DUSt3R の結果をテーブル 1 に示します。私たちの手法は、特徴マッチング型のアプローチ [100, 102] やエンドツーエンドの学習ベースの手法 [11, 54, 101, 124, 151] に対して、比較可能な精度を得ており、場合によっては HLoc [100] のような強力なベースラインを上回る結果を示しています。このことは、以下の二つの理由から重要だと考えています。第一に、DUSt3R は視覚的な位置特定のために一切トレーニングされていません。第二に、クエリ画像やデータベース画像は、DUSt3R のトレーニング中に見られることはありませんでした。



4.2 multi view pose estimation
### 評価方法
DUSt3R の多視点相対ポーズ推定を、セクション 3.4 でのグローバルアラインメント後に評価します。

### データセット
[139] に従い、評価のために二つの多視点データセット、CO3Dv2 [93] および RealEstate10k [185] を使用します。CO3Dv2 には、約 37,000 本の動画から抽出された 600 万フレームが含まれており、51 の MS-COCO カテゴリをカバーしています。真のカメラポーズは、各動画の 200 フレームを用いて COLMAP によってアノテーションされています。RealEstate10k は、YouTube の約 80,000 本のビデオクリップからの 1,000 万フレームを持つ屋内/屋外データセットで、カメラポーズは SLAM とバンドル調整を用いて取得されています。DUSt3R を CO3Dv2 の 41 カテゴリおよび RealEstate10k のテストセットからの 1,800 本のビデオクリップで評価するために、[139] で導入されたプロトコルに従います。各シーケンスごとにランダムに 10 フレームを選択し、可能な 45 ペアを DUSt3R に供給します。

### ベースラインとメトリック
DUSt3R のポーズ推定結果を、PnP-RANSAC またはグローバルアラインメントを用いて取得したものと、学習ベースの RelPose [176]、PoseReg [139]、PoseDiffusion [139] および構造ベースの PixSFM [58]、COLMAP+SPSG（SuperPoint [26] と SuperGlue [99] で拡張された COLMAP [106]）と比較します。[139] に類似して、相対ポーズ誤差を評価するために、各画像ペアに対して相対回転精度 (RRA) と相対平行移動精度 (RTA) を報告し、閾値 τ = 15 を選択して RTA@15 と RRA@15 を報告します。さらに、角度差の下の曲線の精度を定義した平均平均精度 (mAA)@30 を計算します。この mAA は、min(RRA@30, RTA@30) に基づいています。

### 結果
テーブル 2 に示すように、グローバルアラインメントを用いた DUSt3R は、二つのデータセットで最良の全体パフォーマンスを達成し、最先端の PoseDiffusion [139] を大幅に上回ります。さらに、DUSt3R の PnP を使用した結果も、学習ベースおよび構造ベースの既存手法を上回る優れたパフォーマンスを示しています。RealEstate10K における PoseDiffusion の結果は CO3Dv2 でトレーニングされたモデルに基づいていることに注意が必要ですが、RealEstate10K は DUSt3R のトレーニング中には使用されていないため、比較は正当化されると主張します。また、付録（セクション D）では、少ない入力ビュー（3 ～ 10）でのパフォーマンスも報告しており、その場合でも DUSt3R は両方のベンチマークで優れたパフォーマンスを発揮しています。



4.3 monocular depth
### モノキュラー深度推定
このモノキュラータスクでは、同じ入力画像 \(I\) をネットワークに \(F(I, I)\) として与えます。設計上、深度予測は予測された 3D ポイントマップの \(z\) 座標として単純に取得されます。

### データセットとメトリック
DUSt3R の性能を、二つの屋外データセット (DDAD [40], KITTI [35]) および三つの屋内データセット (NYUv2 [114], BONN [79], TUM [118]) でベンチマークします。DUSt3R のパフォーマンスを、教師あり、自己教師あり、ゼロショット設定に分類された最先端の手法と比較します。ゼロショット設定は DUSt3R に該当します。モノキュラー深度評価で一般的に使用される二つのメトリックを用います：ターゲット \(y\) と予測 \( \hat{y} \) の絶対相対誤差 \( \text{AbsRel} = \frac{|y - \hat{y}|}{y} \) と、予測しきい値精度 \( \delta_{1.25} = \max\left(\frac{\hat{y}}{y}, \frac{y}{\hat{y}}\right) < 1.25 \) です。

### 結果
ゼロショット設定における最先端は、最近の SlowTv [116] によって代表されます。このアプローチは、都市、自然、合成、屋内シーンのキュレーションされたデータセットの大規模な混合を収集し、共通のモデルをトレーニングしました。混合に含まれる各データセットでは、カメラパラメータが既知または COLMAP で推定されています。テーブル 2 に示すように、DUSt3R は屋外および屋内環境にうまく適応し、自己教師ありのベースライン [6, 37, 120] を上回り、教師ありの最先端ベースライン [90, 173] と同等の性能を発揮します。



4.4 multi view depth
### マルチビュー ステレオ深度推定

DUSt3R を使用して、マルチビュー ステレオ深度推定タスクを評価します。同様に、深度マップは予測されたポイントマップの \(z\) 座標として抽出されます。複数の深度マップが同じ画像に対して利用可能な場合は、すべての予測をスケーリングして整列させ、信頼度に基づいて単純に平均化して集約します。

### データセットとメトリック
[109] に従って、DUSt3R を DTU [1]、ETH3D [108]、Tanks and Temples [49]、および ScanNet [23] データセットで評価します。各テストセットおよびすべてのテストセットの平均に対して、絶対相対誤差 (rel) としきい値 1.03 に基づくインライヤー比率 (τ) を報告します。なお、地上真実のカメラパラメータやポーズ、地上真実の深度範囲は利用していないため、予測はスケールファクターまでの有効性しかありません。定量的な測定を行うため、予測された深度と地上真実の深度の中央値を使用して予測を正規化します（[109] で推奨）。

### 結果
テーブル 3 に示すように、DUSt3R は ETH-3D で最先端の精度を達成し、最近のほとんどの最先端手法を上回ります。特に、地上真実のカメラポーズを使用する手法に対しても優れた性能を示しています。また、時間的には、DUSt3R のアプローチは従来の COLMAP パイプライン [105, 106] よりもはるかに高速です。これにより、DUSt3R が屋内、屋外、小規模、大規模シーンのさまざまなドメインに適用可能であることが示されています。ただし、テストドメインに関しては、ScanNet テストセットを除き、トレーニングデータに含まれていません。



4.5 3d reconstruction
### フル再構築の品質評価

最後に、セクション 3.4 で説明したグローバルアラインメント手法の後に得られたフル再構築の品質を測定します。私たちの手法は、カメラの内部および外部パラメータに関する事前知識なしにグローバルな非制約型マルチビュー ステレオ (MVS) を可能にする初めての方法であることを再度強調します。再構築の品質を定量化するために、予測を地上真実の座標系に整列させます。これは、セクション 3.4 でパラメータを定数として固定することによって行います。これにより、地上真実の座標系で表現された一貫した 3D 再構築が得られます。

### データセットとメトリック
DTU [1] データセットで予測を評価します。ゼロショット設定でネットワークを適用し、DTU のトレインセットにファインチューニングは行わず、モデルをそのまま使用します。表 4 には、著者が提供する平均精度、平均完全性、全体の平均誤差メトリックを報告します。再構築された形状の点の精度は、地上真実への最小ユークリッド距離として定義され、地上真実の点の完全性は再構築された形状への最小ユークリッド距離として定義されます。全体は単に前述の 2 つのメトリックの平均です。

### 結果
私たちの手法は、最良の手法の精度レベルには達しません。この点に関しては、これらの手法がすべて地上真実のポーズを活用し、適用可能な場合は DTU のトレインセットで特別にトレーニングを行っているためです。さらに、このタスクの最良の結果は通常、サブピクセル精度の三角測量によって得られ、明示的なカメラパラメータの使用が必要です。一方、私たちのアプローチは回帰に依存しており、これは精度が低いことが知られています。それでも、カメラに関する事前知識なしで、平均精度 2.7mm、完全性 0.8mm、全体の平均距離 1.7mm に達しました。このレベルの精度は、私たちのアプローチのプラグアンドプレイ性を考慮すると、実際に非常に有用であると信じています。



4.6 ablations
### CroCo プレトレーニングと画像解像度の影響

私たちは、DUSt3R のパフォーマンスに対する CroCo プレトレーニングおよび画像解像度の影響をアブレーションしました。タスクごとの結果を表 1、表 2、表 3 に報告します。全体的に観察された一貫した改善は、モダンなデータ駆動型アプローチにおけるプレトレーニングと高解像度の重要な役割を示唆しています。これは、文献 [77, 148] でも指摘されています。





5 conclusion
私たちは、シーンやカメラに関する事前情報なしで、野外での3D再構築だけでなく、さまざまな3Dビジョンタスクを解決するための新しいパラダイムを提案しました。