abst
私たちは、単一のRGB画像のみを用いてオブジェクトのカメラ視点を変更するためのフレームワーク「Zero-1-to-3」を紹介します。この制約の少ない設定で新しい視点合成を行うために、私たちは大規模な拡散モデルが自然画像について学習した幾何学的先入観を活用します。私たちの条件付き拡散モデルは、合成データセットを使用して相対的なカメラ視点の制御を学習し、指定されたカメラ変換の下で同じオブジェクトの新しい画像を生成することを可能にします。合成データセットで訓練されているにもかかわらず、私たちのモデルは、分布外データセットや印象派の絵画を含む実世界の画像に対しても強力なゼロショット一般化能力を保持しています。私たちの視点条件付き拡散アプローチは、単一の画像からの3D再構築のタスクにも利用できます。定性的および定量的な実験により、私たちの方法は、インターネット規模の事前学習を活用することによって、最先端の単一視点3D再構築および新しい視点合成モデルを大幅に上回ることが示されています。





1 intro
単一のカメラビューから、人間はしばしばオブジェクトの3D形状や外観を想像することができます。この能力は、物体の操作や複雑な環境でのナビゲーションといった日常的なタスクにおいて重要であり、また絵画などの視覚的創造性にも鍵となります。この能力は対称性などの幾何学的先入観に依存することで部分的に説明できますが、私たちは物理的および幾何学的制約を破るような、より難しいオブジェクトに対しても容易に一般化できるようです。実際、物理的に存在しない、あるいは存在できないオブジェクトの3D形状を予測することさえ可能です。こうした一般化の程度を達成するために、人間は生涯にわたる視覚的探求を通じて蓄積された事前知識に依存しています。

対照的に、既存の3D画像再構築アプローチのほとんどは、高価な3Dアノテーション（例：CADモデル）やカテゴリ特有の先入観に依存しているため、閉じた世界設定で操作されています。最近では、CO3Dなどの大規模で多様なデータセットでの事前学習により、オープンワールド3D再構築の方向性で大きな進展を遂げた手法がいくつか登場しています。しかし、これらのアプローチはしばしば、訓練に必要なジオメトリ関連情報（例：ステレオビューやカメラポーズ）を必要とします。その結果、これらが使用するデータのスケールと多様性は、最近のインターネット規模のテキスト-画像コレクションと比較して重要性に欠けるものとなっています。インターネット規模の事前学習により、これらのモデルには豊かなセマンティック先入観が備わっていることが示されていますが、幾何学的情報をどの程度キャプチャしているかはほとんど探求されていません。

本論文では、大規模な拡散モデル（例：Stable Diffusion）のカメラ視点を操作する制御メカニズムを学習できることを示します。これにより、ゼロショットの新しい視点合成と3D形状再構築を行うことができます。単一のRGB画像が与えられた場合、これらのタスクは非常に制約が少ないものです。しかし、現代の生成モデルに利用可能な訓練データの規模（50億枚以上）のおかげで、拡散モデルは自然画像分布に対する最先端の表現となり、多くの視点からのさまざまなオブジェクトをカバーしています。これらのモデルはカメラ対応なしで2D単眼画像で訓練されていますが、生成プロセス中に相対的なカメラ回転と移動の制御を学習するようにモデルを微調整することができます。これらの制御により、任意の画像をエンコードし、選択した異なるカメラ視点にデコードすることが可能になります。図1には、我々の結果のいくつかの例が示されています。

本論文の主な貢献は、大規模な拡散モデルが2D画像のみで訓練されているにもかかわらず、視覚的世界に関する豊かな3D先入観を学習していることを示すことです。また、単一のRGB画像からの新しい視点合成とゼロショット3D再構築の両方において、最先端の結果を実現したことを示します。以下のように構成されています。

1. **関連研究のレビュー**：セクション2では、関連する研究を簡単にレビューします。
  
2. **カメラ外部パラメータの制御を学習するアプローチ**：セクション3では、大規模な拡散モデルを微調整することによって、カメラの外部パラメータの制御を学習するアプローチを説明します。

3. **定量的および定性的な実験の提示**：セクション4では、単一の画像からの視点合成と3D再構築の幾何学と外観を評価するためのいくつかの定量的および定性的な実験を提示します。

最後に、すべてのコードとモデル、およびオンラインデモを公開する予定です。





2 related work
### 3D生成モデル

最近の生成画像アーキテクチャの進展により、大規模な画像-テキストデータセットと組み合わせて、高忠実度の多様なシーンやオブジェクトを合成することが可能になりました。特に、拡散モデルは、ノイズ除去の目的を使用してスケーラブルな画像生成器を学習する上で非常に効果的であることが示されています。しかし、これらを3Dドメインにスケーリングするには、大量の高価な注釈付き3Dデータが必要です。そこで、最近のアプローチは、グラウンドトゥルースの3Dデータを使用せずに、事前に訓練された大規模2D拡散モデルを3Dに転送することに依存しています。

Neural Radiance Fields（NeRF）は、その高忠実度でシーンをエンコードできる能力により、強力な表現として登場しました。通常、NeRFは単一のシーン再構築に使用され、シーン全体をカバーする多くのポーズ付き画像が提供されます。タスクは、未観測の角度からの新しいビューを予測することです。DreamFieldsは、NeRFが3D生成システムの主要なコンポーネントとしても使用できることを示しました。さまざまなフォロワークは、CLIPを使用して2D拡散モデルからの蒸留損失に置き換え、高忠実度の3Dオブジェクトやシーンをテキスト入力から生成することを目指しています。

本研究では、新しい視点合成を拡散モデルを使用した視点条件付きの画像間変換タスクとしてモデル化するという非伝統的なアプローチを探求します。学習したモデルは、単一の画像から3D形状を再構築するために3D蒸留と組み合わせることもできます。以前の研究では、類似のパイプラインを採用しましたが、ゼロショット一般化能力を示しませんでした。同時に、画像から3D生成を実行するための言語ガイドの先入観やテキストの逆転を用いた類似の技術を提案したアプローチもあります。しかし、我々の手法は、合成データセットを通じて視点の制御を学習し、野外の画像に対してゼロショットの一般化を示します。

### 単一ビューオブジェクト再構築

単一のビューから3Dオブジェクトを再構築することは、強力な先入観を必要とする非常に挑戦的な問題です。ある種の研究は、メッシュ、ボクセル、またはポイントクラウドとして表現された3Dプリミティブのコレクションから先入観を構築し、画像エンコーダを条件付けに使用します。これらのモデルは、使用される3Dデータコレクションの多様性によって制約を受けており、グローバルな条件付けの性質により一般化能力が乏しいです。さらに、推定された形状と入力との間の整合性を確保するために、追加のポーズ推定ステップが必要です。一方で、局所的に条件付けられたモデルは、シーン再構築のために局所画像特徴を直接使用し、より優れたクロスドメイン一般化能力を示しますが、一般的には近くのビューの再構築に制限されます。

最近のMCCは、RGB-Dビューからの3D再構築のための汎用表現を学習し、オブジェクト中心のビデオの大規模データセットで訓練されています。本研究では、事前訓練されたStable Diffusionモデルから豊富な幾何学的情報を直接抽出できることを示し、追加の深度情報の必要性を軽減します。





3 method
与えられた単一のRGB画像 \( x \in \mathbb{R}^{H \times W \times 3} \) のオブジェクトに対し、異なるカメラ視点からの画像を合成することを目指します。ここで、 \( R \in \mathbb{R}^{3 \times 3} \) と \( T \in \mathbb{R}^{3} \) は、望ましい視点の相対的なカメラ回転と平行移動を表します。私たちは、次のように新しい画像を合成するモデル \( f \) を学習することを目指しています：

\[
\hat{x}_{R,T} = f(x, R, T) \quad (1)
\]

ここで、\( \hat{x}_{R,T} \) は合成された画像を示します。我々は、推定された \( \hat{x}_{R,T} \) が、真の未観測の新しい視点 \( x_{R,T} \) に知覚的に類似していることを望みます。

単眼RGB画像からの新しい視点合成は、非常に制約の少ない問題です。我々のアプローチは、Stable Diffusionのような大規模な拡散モデルを活用してこのタスクを実行します。これらのモデルは、テキスト記述から多様な画像を生成する際に驚異的なゼロショット能力を示します。彼らのトレーニングデータのスケールにより、事前に訓練された拡散モデルは、今日の自然画像分布に対する最先端の表現となっています。

しかし、\( f \) を作成するためには克服すべき2つの課題があります。まず第一に、大規模生成モデルは異なる視点のさまざまなオブジェクトで訓練されていますが、視点間の対応関係を明示的にエンコードしていません。第二に、生成モデルはインターネット上で反映される視点バイアスを引き継ぎます。図2に示すように、Stable Diffusionは、正面を向いた椅子の標準的なポーズの画像を生成する傾向があります。これらの2つの問題は、大規模な拡散モデルから3D知識を抽出する能力を大いに妨げます。



3.1 learning to control camera viewpoint
拡散モデルはインターネットスケールのデータで訓練されているため、ほとんどのオブジェクトに対して多くの視点をカバーしていると考えられます。しかし、事前に訓練されたモデルではこれらの視点を制御することはできません。写真が撮影された際のカメラの外部パラメータ（視点）を制御するメカニズムをモデルに教えることができれば、新しい視点からの画像合成が可能になります。

この目的のために、相対的なカメラの外部パラメータとペアになった画像データセット \( \{x, x_{(R,T)}, R, T\} \) を用いて、図3に示すように、事前に訓練された拡散モデルを微調整し、カメラパラメータの制御を学習します。これにより、元の画像生成の表現を損なうことなく、視点の制御を可能にします。手法としては、[44] に従い、エンコーダー \( E \)、ノイズ除去ネットワーク U-Net \( \epsilon_\theta \)、デコーダー \( D \) を備えた潜在拡散アーキテクチャを使用します。拡散の時間ステップ \( t \sim [1, 1000] \) において、入力ビューと相対的なカメラパラメータの埋め込み \( c(x, R, T) \) を定義します。その後、次の目的を解いてモデルを微調整します。

\[
\min_\theta \mathbb{E}_{z \sim E(x), t, \epsilon \sim N(0,1)} \left[ \| \epsilon - \epsilon_\theta(z_t, t, c(x, R, T)) \|_2^2 \right] \quad (2)
\]

モデル \( \epsilon_\theta \) の訓練が完了すると、推論モデル \( f \) は、ガウスノイズ画像から \( c(x, R, T) \) に条件付けられた状態で反復的にノイズを除去し、画像を生成します【44】。

この論文の主要な成果は、この方法で事前に訓練された拡散モデルを微調整すると、カメラの視点を制御する汎用的なメカニズムを学習できるという点です。これにより、微調整データセットに含まれていないオブジェクトにも外挿して視点制御が可能になります。つまり、この微調整によって、視点の制御が「追加」され、生成された画像は依然としてフォトリアリスティックでありながら、視点制御の能力を獲得します。この合成能力によって、3Dアセットが存在せず、微調整セットに含まれていないオブジェクトクラスに対しても、新しい視点からの画像をゼロショットで生成できるようになります。



3.2 view conditioned diffusion
単一の画像からの3D再構成には、低レベルの知覚（深度、シェーディング、テクスチャなど）と高レベルの理解（タイプ、機能、構造など）の両方が必要です。そこで、ハイブリッド条件付けメカニズムを採用します。

一方のストリームでは、入力画像のCLIP埋め込みを相対的なカメラの回転 \( R \) と平行移動 \( T \) と連結し、「ポーズ付きCLIP」埋め込み \( c(x, R, T) \) を形成します。この埋め込みは、ノイズ除去用のU-Netに対してクロスアテンションを適用し、入力画像の高レベルな意味情報を提供します。

もう一方のストリームでは、入力画像をチャネルで結合し、ノイズ除去される画像と結合します。これにより、合成されるオブジェクトのアイデンティティや詳細を保持するのを助けます。

また、分類器なしのガイダンス（classifier-free guidance）を適用できるように、[3] で提案された類似のメカニズムに従い、推論時に入力画像とポーズ付きCLIP埋め込みをランダムにゼロベクトルに設定し、条件情報をスケーリングします。このアプローチにより、低レベルの詳細を保持しつつ、高レベルの意味を効果的に活用できるようになります。



3.3 3d reconstruction
多くのアプリケーションでは、オブジェクトの新しい視点を合成するだけでは不十分であり、オブジェクトの外観とジオメトリの両方を捉えた完全な3D再構成が求められます。そこで、私たちは最近オープンソース化されたフレームワークであるScore Jacobian Chaining（SJC）を採用し、テキストから画像への拡散モデルから得られる先行知識を使って3D表現を最適化します。

しかし、拡散モデルの確率的性質により、勾配の更新は非常に確率的になります。SJCで使用される重要な技術は、DreamFusionから着想を得て、分類器なしのガイダンス値を通常よりもかなり高く設定することです。この方法論により、各サンプルの多様性は低下しますが、再構成の忠実度が向上します。

図4に示すように、SJCと同様に、視点をランダムにサンプリングし、体積レンダリングを行います。次に、得られた画像にガウスノイズ \( \epsilon \sim N(0, 1) \) を加え、入力画像 \( x \)、ポーズ付きCLIP埋め込み \( c(x, R, T) \)、および時間ステップ \( t \) に条件付けたU-Net \( \epsilon_\theta \) を適用してデノイズし、ノイズのない入力 \( x_\pi \) に対するスコアを近似します：

\[
\nabla L_{SJC} = \nabla I_\pi \log p_{\sqrt{2\epsilon}}(x_\pi) \quad (3)
\]

ここで、\( \nabla L_{SJC} \) は[53]で導入されたPAASスコアです。

さらに、入力ビューを平均二乗誤差（MSE）損失で最適化します。NeRF表現をさらに正則化するために、サンプリングされた各視点に対して深度平滑性損失を適用し、近接視点間の外観の変化を正則化するために近距離一貫性損失を適用します。これにより、3D再構成の品質が向上し、異なる視点での整合性が強化されます。



3.4 dataset
私たちは最近リリースされたObjaverseデータセットをファインチューニングに使用します。このデータセットは、100,000人以上のアーティストによって作成された80万以上の3Dモデルを含む大規模なオープンソースデータセットです。ShapeNetのような明示的なクラスラベルはありませんが、Objaverseは高品質な3Dモデルの大きな多様性を体現しており、豊かなジオメトリや細かいディテール、マテリアル特性を持つモデルが多く存在します。

データセット内の各オブジェクトについて、中心を指す12個のカメラ外部行列 \( M_e \) をランダムにサンプリングし、レイトレーシングエンジンを用いて12のビューをレンダリングします。トレーニング時には、各オブジェクトに対して2つのビューをサンプリングして画像ペア \( (x, x_{R,T}) \) を形成します。両方の視点間のマッピングを定義する対応する相対視点変換 \( (R, T) \) は、2つの外部行列から簡単に導出できます。この方法により、Objaverseデータセットの豊富な情報を活用して、視点条件付きの画像合成が可能になります。





4 experiments
私たちは、ゼロショットの新しい視点合成と3D再構築に関するモデルのパフォーマンスを評価します。Objaverseの著者によって確認されたように、本論文で使用したデータセットと画像はObjaverseデータセットの外にあるため、ゼロショットの結果と見なすことができます。合成オブジェクトやシーンの異なる複雑さレベルに対して、私たちのモデルを最先端技術と定量的に比較します。また、日常の物体の写真から絵画に至るまで、多様な野生の画像を用いた定性的結果も報告します。

この評価により、私たちのアプローチが実世界のデータに対しても優れた性能を発揮し、ゼロショットの状況においても信頼性が高いことを示すことができます。



4.1 tasks
私たちは、単一のRGB画像を入力とする2つの密接に関連したタスクについて説明し、ゼロショットで適用します。

### 新しい視点合成
新しい視点合成は、コンピュータビジョンにおける古くからの3D問題であり、モデルがオブジェクトの深度、テクスチャ、形状を暗黙的に学習することを要求します。単一の視点からの情報は非常に限られているため、新しい視点合成手法は事前知識を活用する必要があります。最近の人気のある手法は、ランダムにサンプリングされた視点からのCLIP一貫性オブジェクト目的を用いて暗黙的ニューラルフィールドを最適化することに依存しています。

しかし、私たちの視点条件付き画像生成のアプローチは、3D再構築と新しい視点合成の順序を逆転させながら、入力画像に描かれたオブジェクトのアイデンティティを保持します。この方法では、オブジェクトの周りを回転する際の自己遮蔽による偶発的な不確実性を、確率的生成モデルによってモデル化でき、また、大規模な拡散モデルが学習したセマンティックおよびジオメトリックな事前情報を効果的に活用することができます。

### 3D再構築
私たちは、SJC [53]やDreamFusion [38]のような確率的な3D再構築フレームワークを適用して、最も可能性の高い3D表現を作成することもできます。これをボクセル放射フィールド [5, 49, 13] としてパラメータ化し、その後、密度フィールドに対してマーチングキューブ法を適用してメッシュを抽出します。

私たちの視点条件付き拡散モデルを3D再構築に適用することで、拡散モデルが学習した豊富な2D外観の事前知識を3Dジオメトリに向けてチャネルする実行可能な道筋を提供します。



4.2 baselines
私たちの手法の範囲と一貫性を保つために、ゼロショット設定で動作し、単一のRGB画像を入力として使用する方法のみを比較します。

### 新規視点合成
新規視点合成においては、いくつかの最先端の単一画像アルゴリズムと比較します。特に、DietNeRF [23] をベンチマークとして使用します。これは、視点間でのCLIP画像対画像一貫性損失でNeRFを正則化する手法です。また、画像ではなくテキストプロンプトに条件付けられたStable DiffusionモデルであるImage Variations (IV) [1]とも比較します。IVは、Stable Diffusionによる意味的な最近傍検索エンジンとも見なせます。最後に、元のテキスト条件付き拡散モデルを画像条件付き拡散モデルに置き換えた拡散ベースのテキストから3Dモデルを生成するモデルであるSJC [53]をSJC-Iと呼び、適応させて比較します。

### 3D再構築
3D再構築のために、次の2つの最先端の単一ビューアルゴリズムをベースラインとして使用します：
1. **Multiview Compressive Coding (MCC)** [59]：RGB-D観測を3D表現に補完するニューラルフィールドベースのアプローチです。
2. **Point-E** [34]：色付けされた点群上の拡散モデルです。MCCはCO3Dv2 [43]で訓練されており、Point-EはOpenAIの内部3Dデータセットで大規模に訓練されています。また、SJC-Iとも比較します。

MCCは深度入力を必要とするため、MiDaS [42, 41]を用いてオフザ shelfの深度推定を行います。得られた相対的な視差マップを、テストセット全体で合理的に見える標準的なスケールとシフト値を仮定することで、絶対的な擬似メトリック深度マップに変換します。



4.3 benchmarks and metrics
私たちは、Google Scanned Objects (GSO) [10] と RTMV [50] というデータセットを用いて両方のタスクを評価します。GSOは高品質なスキャン済みの家庭用品のデータセットであり、RTMVは20のランダムなオブジェクトから構成される複雑なシーンで構成されています。すべての実験において、それぞれのグラウンドトゥルース3Dモデルが3D再構築の評価に使用されます。

### 新規視点合成の評価
新規視点合成に関しては、我々の手法とベースラインを数値的に広範囲に評価し、画像の類似性の異なる側面をカバーする4つのメトリックを用います：
- **PSNR (Peak Signal-to-Noise Ratio)**：信号の最大パワーとノイズのパワーの比を示す指標。
- **SSIM (Structural Similarity Index)** [55]：画像の構造的な類似性を評価するための指標。
- **LPIPS (Learned Perceptual Image Patch Similarity)** [64]：視覚的に類似したパッチを評価するために学習された指標。
- **FID (Fréchet Inception Distance)** [18]：生成された画像と実際の画像の分布の距離を測定する指標。

### 3D再構築の評価
3D再構築においては、以下の指標を用いて評価を行います：
- **Chamfer Distance**：2つの点群の間の距離を測定し、形状の一致度を評価します。
- **Volumetric IoU (Intersection over Union)**：再構築された3D形状とグラウンドトゥルースの体積の重なり具合を測定します。



4.4 novel view synthesis results
私たちは数値結果を表1と表2に示しています。図5では、私たちの手法がGSOにおけるすべてのベースラインと比較して、高度にフォトリアリスティックな画像を生成し、グラウンドトゥルースと密接に一致していることが示されています。この傾向は、Objaverseデータセットとは異なるシーンであるRTMVにおいても図6で見られます。

### ベースラインとの比較
ベースラインの中では、Point-Eが他の手法よりもはるかに良い結果を達成する傾向があり、優れたゼロショットの一般化能力を維持しています。ただし、生成されたポイントクラウドのサイズが小さいため、Novel view synthesisへの応用が大きく制限されています。

### 一般化性能のデモ
図7では、挑戦的なジオメトリやテクスチャを持つオブジェクトに対する私たちのモデルの一般化性能をさらに示し、オブジェクトのタイプ、アイデンティティ、低レベルの詳細を維持しながら、高忠実度の視点を合成する能力を確認しました。

### サンプル間の多様性
単一画像からの新しい視点の合成は非常に制約の多いタスクであり、これはDiffusionモデルがNeRFよりも基盤となる不確実性を捉えるための特に適したアーキテクチャであることを意味しています。入力画像が2Dであるため、常にオブジェクトの部分的なビューしか示されず、多くの部分が観察されていない状態になります。図8は、新しい視点からサンプリングされた、高品質な画像の多様性を例示しています。



4.5 3d reconstruction results
私たちは表3と表4に数値結果を示しています。図9では、私たちの手法がグラウンドトゥルースと一貫した高忠実度の3Dメッシュを再構築していることが定性的に示されています。

### ベースラインの比較
- **MCC**は、入力ビューから見える表面の良好な推定を行う傾向がありますが、オブジェクトの背面のジオメトリを正しく推定することができないことがよくあります。
- **SJC-I**もまた、有意義なジオメトリを再構築することができないことが頻繁にあります。
- 一方で、**Point-E**は印象的なゼロショットの一般化能力を持ち、オブジェクトジオメトリの合理的な推定を予測することができます。しかし、彼らの提供するメッシュ変換方法によれば、4,096点の非一様なスパースポイントクラウドを生成するため、再構築された表面にホールが生じることがあります。そのため、良好なChamfer Distance（CD）スコアを得るものの、体積IoU（volumetric IoU）には不十分です。

### 我々の手法の強み
私たちの手法は、視点条件付きのDiffusionモデルから学習したマルチビューの事前知識を活用し、それをNeRFスタイルの表現の利点と組み合わせています。この二つの要因により、表3と表4で示されるように、従来の手法に対してCDおよび体積IoUの面で改善をもたらしています。



4.6 text to image tp 3D
自然画像だけでなく、私たちはDall-E-2などのtxt2imgモデルによって生成された画像でも手法をテストしました。図10に示すように、私たちのモデルはこれらの画像の新しい視点を生成しつつ、オブジェクトのアイデンティティを保持することができます。この特性は、多くのテキストから3D生成アプリケーションにおいて非常に有用であると考えています。





5 discussion
本研究では、ゼロショットの単一画像による新しい視点合成と3D再構築のための新しいアプローチ「Zero1-to-3」を提案しました。我々の手法は、インターネット規模のデータで事前訓練されたStable Diffusionモデルを活用し、豊富な意味的および幾何学的事前知識を捉えています。この情報を抽出するために、我々は合成データでモデルをファインチューニングし、カメラ視点を制御する方法を学習しました。その結果、Stable Diffusionによって学習された強力なオブジェクト形状の事前知識を活用できるため、いくつかのベンチマークで最先端の結果を示しました。



5.1 future work
### オブジェクトからシーンへ
我々のアプローチは、単一のオブジェクトが平面の背景に配置されたデータセットで訓練されています。RTMVデータセットでは、複数のオブジェクトがあるシーンへの強い一般化能力を示しましたが、GSOからの分布内サンプルに比べて品質が劣化します。したがって、複雑な背景を持つシーンへの一般化は、我々の手法にとって重要な課題として残っています。

### シーンからビデオへ
単一の視点から動的シーンの幾何学を理解できるようになれば、オクルージョンの理解や動的オブジェクトの操作など、新しい研究方向が開けます。最近、拡散ベースのビデオ生成に関するいくつかのアプローチが提案されていますが、これらを3Dに拡張することが重要です。

### Stable Diffusionとグラフィックスパイプラインの統合
本論文では、Stable Diffusionからオブジェクトの3D知識を抽出するフレームワークを示しました。Stable Diffusionのような強力な自然画像生成モデルには、照明、シェーディング、テクスチャなどに関する他の暗黙の知識も含まれています。今後の研究では、シーンの再照明などの従来のグラフィックスタスクを実行するための類似のメカニズムを探求できるでしょう。

### 謝辞
Changxi ZhengとSamir Gadreの貴重なフィードバックに感謝します。また、SJC、NeRDi、SparseFusion、Objaverseの著者たちにも有益な議論に感謝します。本研究は、トヨタ研究所、DARPA MCSプログラム（連邦契約番号N660011924032）、およびNSF NRI賞#1925157によって部分的に支援されています。